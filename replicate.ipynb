{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChildID</th>\n",
       "      <th>AFQT_Pct81</th>\n",
       "      <th>AFQT_Pct81_REV</th>\n",
       "      <th>AgePreg94</th>\n",
       "      <th>AgePreg96</th>\n",
       "      <th>AgePreg98</th>\n",
       "      <th>Age_1stHS88</th>\n",
       "      <th>Age_1stHS90</th>\n",
       "      <th>Age_1stHS92</th>\n",
       "      <th>Age_1stHS94</th>\n",
       "      <th>...</th>\n",
       "      <th>YA_LD104</th>\n",
       "      <th>YA_LastInterview</th>\n",
       "      <th>YA_NumKids</th>\n",
       "      <th>YA_Res94</th>\n",
       "      <th>YA_Res96</th>\n",
       "      <th>YA_Res98</th>\n",
       "      <th>YA_Res100</th>\n",
       "      <th>YA_Res102</th>\n",
       "      <th>YA_Res104</th>\n",
       "      <th>hhID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>301.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>302.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>303.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11465</th>\n",
       "      <td>1267201.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11466</th>\n",
       "      <td>1267202.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12672.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11467</th>\n",
       "      <td>1267301.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12673.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11468</th>\n",
       "      <td>1267302.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12673.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11469</th>\n",
       "      <td>1267501.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12675.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11470 rows Ã— 882 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ChildID  AFQT_Pct81  AFQT_Pct81_REV  AgePreg94  AgePreg96  AgePreg98  \\\n",
       "0          201.0        12.0             9.0        NaN        NaN        NaN   \n",
       "1          202.0        12.0             9.0        NaN        NaN        NaN   \n",
       "2          301.0        51.0            46.0        NaN        NaN        NaN   \n",
       "3          302.0        51.0            46.0        NaN        NaN        NaN   \n",
       "4          303.0        51.0            46.0        NaN        NaN        NaN   \n",
       "...          ...         ...             ...        ...        ...        ...   \n",
       "11465  1267201.0        64.0            63.0        NaN        NaN        NaN   \n",
       "11466  1267202.0        64.0            63.0        NaN        NaN        NaN   \n",
       "11467  1267301.0        80.0            80.0        NaN        NaN        NaN   \n",
       "11468  1267302.0        80.0            80.0        NaN        NaN        NaN   \n",
       "11469  1267501.0        67.0            50.0        NaN        NaN        NaN   \n",
       "\n",
       "       Age_1stHS88  Age_1stHS90  Age_1stHS92  Age_1stHS94  ...  YA_LD104  \\\n",
       "0              NaN          NaN          NaN          NaN  ...       NaN   \n",
       "1              NaN          NaN          NaN          NaN  ...       NaN   \n",
       "2              NaN          NaN          NaN          NaN  ...       NaN   \n",
       "3              NaN          NaN          NaN          NaN  ...       NaN   \n",
       "4              NaN          NaN          NaN          NaN  ...       NaN   \n",
       "...            ...          ...          ...          ...  ...       ...   \n",
       "11465          NaN          NaN          NaN          NaN  ...       NaN   \n",
       "11466          NaN          NaN          NaN          NaN  ...       NaN   \n",
       "11467          NaN          NaN          NaN          NaN  ...       NaN   \n",
       "11468          NaN          NaN          NaN          NaN  ...       NaN   \n",
       "11469          NaN          NaN          NaN          NaN  ...       NaN   \n",
       "\n",
       "       YA_LastInterview  YA_NumKids  YA_Res94  YA_Res96  YA_Res98  YA_Res100  \\\n",
       "0                   NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "1                   NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "2                2002.0         0.0       NaN      11.0      19.0        NaN   \n",
       "3                2002.0         0.0       NaN       NaN      19.0       11.0   \n",
       "4                2002.0         0.0       NaN       NaN       NaN        NaN   \n",
       "...                 ...         ...       ...       ...       ...        ...   \n",
       "11465               NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "11466               NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "11467               NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "11468               NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "11469               NaN         NaN       NaN       NaN       NaN        NaN   \n",
       "\n",
       "       YA_Res102  YA_Res104     hhID  \n",
       "0            NaN        NaN      2.0  \n",
       "1            NaN        NaN      2.0  \n",
       "2           19.0        NaN      3.0  \n",
       "3           11.0        NaN      3.0  \n",
       "4           19.0        NaN      3.0  \n",
       "...          ...        ...      ...  \n",
       "11465        NaN        NaN  12672.0  \n",
       "11466        NaN        NaN  12672.0  \n",
       "11467        NaN        NaN  12673.0  \n",
       "11468        NaN        NaN  12673.0  \n",
       "11469        NaN        NaN  12675.0  \n",
       "\n",
       "[11470 rows x 882 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_stata('data_Deming_2008_0217.dta')\n",
    "data[data < 0] = np.nan # replace negative values with NaN\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'num_eligible_children_{year}'] = grouped_data[f'cleaned_age_mo_{year}'].transform(lambda x: x[(x >= 54) & (x.notna())].count()) # 4 yr 6 months\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[eligibility_mask, f'eligibility_{year}'] = 1\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['deceased'] = data['Res104'] == 8\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['headstart_participation_90'] = headstart_participation_mask.apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['preschool_participation_90'] = (preschool_participation_mask & ~headstart_participation_mask).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['no_preschool_participation_90'] = (~preschool_participation_mask & ~headstart_participation_mask).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'eligibility_siblingdifferenttreatment_{year}'] = ((hs_indicator + pr_indicator + no_indicator) >= 2) & (data[f'eligibility_{year}'] == 1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:50: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'headstart_fixedeffect_indicator_{year}'] = data[f'headstart_participation_{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'preschool_fixedeffect_indicator_{year}'] = data[f'preschool_participation_{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'no_preschool_fixedeffect_indicator_{year}'] = data[f'no_preschool_participation_{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"program_participation_type_{year}\"] = data[f'headstart_participation_{year}'] + 2 * data[f'preschool_participation_{year}'] + 3 * data[f'no_preschool_participation_{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/2578242216.py:62: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f\"program_participation_fixedeffect_type_{year}\"] = data[f'headstart_fixedeffect_indicator_{year}'] + 2 * data[f'preschool_fixedeffect_indicator_{year}'] + 3 * data[f'no_preschool_fixedeffect_indicator_{year}']\n"
     ]
    }
   ],
   "source": [
    "# Eligibility\n",
    "\n",
    "# age\n",
    "survey_years = list(range(86, 106, 2))\n",
    "for year in survey_years:\n",
    "    data[f'cleaned_age_mo_{year}'] = data[f'Age_Mo{year}']\n",
    "    data[f'cleaned_age_mo_{year}'].fillna(data[f'PPVTAge{year}'], inplace=True)\n",
    "for idx, year in enumerate(survey_years):\n",
    "    if idx != 0:\n",
    "        mask = data[f'cleaned_age_mo_{year}'].isna() & data[f'cleaned_age_mo_{year-2}'].notna()\n",
    "        data.loc[mask, f'cleaned_age_mo_{year}'] = data.loc[mask, f'cleaned_age_mo_{year-2}'] + 24\n",
    "    if idx != len(survey_years) - 1:\n",
    "        mask = data[f'cleaned_age_mo_{year}'].isna() & data[f'cleaned_age_mo_{year+2}'].notna() & data[f'cleaned_age_mo_{year+2}'] >= 25\n",
    "        data.loc[mask, f'cleaned_age_mo_{year}'] = data.loc[mask, f'cleaned_age_mo_{year+2}'] - 24\n",
    "for year in survey_years:\n",
    "    data[f'cleaned_age_yr_{year}'] = data[f'cleaned_age_mo_{year}'] // 12\n",
    "\n",
    "# each household must have at least two eligible children\n",
    "grouped_data = data.groupby('MotherID')\n",
    "for year in [90]:\n",
    "    data[f'num_eligible_children_{year}'] = grouped_data[f'cleaned_age_mo_{year}'].transform(lambda x: x[(x >= 54) & (x.notna())].count()) # 4 yr 6 months\n",
    "    eligibility_mask = (data[f'num_eligible_children_{year}'] > 1) & (data[f'num_eligible_children_{year}'].notna())\n",
    "    data.loc[eligibility_mask, f'eligibility_{year}'] = 1\n",
    "    data.loc[~eligibility_mask, f'eligibility_{year}'] = 0\n",
    "\n",
    "    death_mask = data[f'Res{year}'] == 8\n",
    "    data.loc[death_mask, f'eligibility_{year}'] = np.nan\n",
    "data['deceased'] = data['Res104'] == 8\n",
    "\n",
    "# program participation\n",
    "headstart_participation_mask = (data['Ever_HS90'] == 1) | (data['Ever_HS88'] == 1)\n",
    "preschool_participation_mask = (data['Ever_Preschool90'] == 1) | (data['Ever_Preschool88'] == 1)\n",
    "eligibility_mask = data['eligibility_90'] == 1\n",
    "data['headstart_participation_90'] = headstart_participation_mask.apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[~eligibility_mask, 'headstart_participation_90'] = np.nan\n",
    "data['preschool_participation_90'] = (preschool_participation_mask & ~headstart_participation_mask).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[~eligibility_mask, 'preschool_participation_90'] = np.nan\n",
    "data['no_preschool_participation_90'] = (~preschool_participation_mask & ~headstart_participation_mask).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[~eligibility_mask, 'no_preschool_participation_90'] = np.nan\n",
    "\n",
    "# within family treatment difference\n",
    "grouped_data = data.groupby('MotherID')\n",
    "year = 90\n",
    "\n",
    "hs_indicator = grouped_data[f'headstart_participation_{year}'].transform(lambda x: 1 if x.sum() > 0 else 0)\n",
    "pr_indicator = grouped_data[f'preschool_participation_{year}'].transform(lambda x: 1 if x.sum() > 0 else 0)\n",
    "no_indicator = grouped_data[f'no_preschool_participation_{year}'].transform(lambda x: 1 if x.sum() > 0 else 0)\n",
    "\n",
    "data[f'eligibility_siblingdifferenttreatment_{year}'] = ((hs_indicator + pr_indicator + no_indicator) >= 2) & (data[f'eligibility_{year}'] == 1)\n",
    "data[f'headstart_fixedeffect_indicator_{year}'] = data[f'headstart_participation_{year}']\n",
    "data[f'preschool_fixedeffect_indicator_{year}'] = data[f'preschool_participation_{year}']\n",
    "data[f'no_preschool_fixedeffect_indicator_{year}'] = data[f'no_preschool_participation_{year}']\n",
    "data.loc[~data[f'eligibility_siblingdifferenttreatment_{year}'], f'headstart_fixedeffect_indicator_{year}'] = np.nan\n",
    "data.loc[~data[f'eligibility_siblingdifferenttreatment_{year}'], f'preschool_fixedeffect_indicator_{year}'] = np.nan\n",
    "data.loc[~data[f'eligibility_siblingdifferenttreatment_{year}'], f'no_preschool_fixedeffect_indicator_{year}'] = np.nan\n",
    "data[f'eligibility_siblingdifferenttreatment_{year}'] = data[f'eligibility_siblingdifferenttreatment_{year}'].apply(lambda x: x if np.isnan(x) else int(x))\n",
    "\n",
    "\n",
    "data[f\"program_participation_type_{year}\"] = data[f'headstart_participation_{year}'] + 2 * data[f'preschool_participation_{year}'] + 3 * data[f'no_preschool_participation_{year}']\n",
    "data.loc[data[f'eligibility_{year}'] == 0, f\"program_participation_type_{year}\"] = np.nan\n",
    "\n",
    "data[f\"program_participation_fixedeffect_type_{year}\"] = data[f'headstart_fixedeffect_indicator_{year}'] + 2 * data[f'preschool_fixedeffect_indicator_{year}'] + 3 * data[f'no_preschool_fixedeffect_indicator_{year}']\n",
    "data.loc[data[f'eligibility_siblingdifferenttreatment_{year}'] == 0, f\"program_participation_fixedeffect_type_{year}\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Hispanic'] = (data['Race_Child'] == 1).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Black'] = (data['Race_Child'] == 2).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['White'] = (data['Race_Child'] == 3).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['NonBlack'] = ((data['Race_Child'] != 2) & (data['Race_Child'].notna())).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['permanent_family_income'] = data[['NetFamInc78', 'NetFamInc79', 'NetFamInc80', 'NetFamInc81', 'NetFamInc82', 'NetFamInc83', 'NetFamInc84',\n",
      "/Users/zekai.wang/anaconda3/envs/stat256/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['log_permanent_family_income'] = np.log(data['permanent_family_income'])\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['permanent_family_income_std'] = (data['permanent_family_income'] - data['permanent_family_income'].mean()) / data['permanent_family_income'].std() # standardize\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].replace(95, np.nan, inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['mother_dropout'] = (mother_edu < 12).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:45: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['mother_highschool'] = (mother_edu == 12).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:47: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['mother_somecollege'] = (mother_edu >= 13).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:51: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['age_adjusted_AFQT'] = data['AFQT_Pct81_REV']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:65: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['age_adjusted_AFQT_std'] = (data['age_adjusted_AFQT'] - data['age_adjusted_AFQT'].mean()) / data['age_adjusted_AFQT'].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1616015331.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['impAFQT_std'] = data['age_adjusted_AFQT_std']\n"
     ]
    }
   ],
   "source": [
    "# Covariates\n",
    "\n",
    "# race\n",
    "data['Hispanic'] = (data['Race_Child'] == 1).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data['Black'] = (data['Race_Child'] == 2).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data['White'] = (data['Race_Child'] == 3).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data['NonBlack'] = ((data['Race_Child'] != 2) & (data['Race_Child'].notna())).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "\n",
    "# family income, adjust to 2004 dollars\n",
    "data['NetFamInc78'] *= 2.82\n",
    "data['NetFamInc79'] *= 2.54\n",
    "data['NetFamInc80'] *= 2.24\n",
    "data['NetFamInc81'] *= 2.03\n",
    "data['NetFamInc82'] *= 1.90\n",
    "data['NetFamInc83'] *= 1.85\n",
    "data['NetFamInc84'] *= 1.78\n",
    "data['NetFamInc85'] *= 1.71\n",
    "data['NetFamInc86'] *= 1.68\n",
    "data['NetFamInc87'] *= 1.62\n",
    "data['NetFamInc88'] *= 1.55\n",
    "data['NetFamInc89'] *= 1.48\n",
    "data['NetFamInc90'] *= 1.41\n",
    "data['NetFamInc91'] *= 1.35\n",
    "data['NetFamInc92'] *= 1.31\n",
    "data['NetFamInc93'] *= 1.27\n",
    "data['NetFamInc95'] *= 1.21\n",
    "data['NetFamInc97'] *= 1.15\n",
    "data['NetFamInc99'] *= 1.10\n",
    "data['NetFamInc101'] *= 1.04\n",
    "\n",
    "data['permanent_family_income'] = data[['NetFamInc78', 'NetFamInc79', 'NetFamInc80', 'NetFamInc81', 'NetFamInc82', 'NetFamInc83', 'NetFamInc84',\n",
    "                                        'NetFamInc85', 'NetFamInc86', 'NetFamInc87', 'NetFamInc88', 'NetFamInc89', 'NetFamInc90', 'NetFamInc91',\n",
    "                                        'NetFamInc92', 'NetFamInc93', 'NetFamInc95', 'NetFamInc97', 'NetFamInc99', 'NetFamInc101']].mean(axis=1)\n",
    "data['log_permanent_family_income'] = np.log(data['permanent_family_income'])\n",
    "data['permanent_family_income_std'] = (data['permanent_family_income'] - data['permanent_family_income'].mean()) / data['permanent_family_income'].std() # standardize\n",
    "\n",
    "# mother's education\n",
    "high_grade_moth_cols = [col for col in data.columns if col.startswith('HighGrade_Moth')]\n",
    "for col in high_grade_moth_cols:\n",
    "    data[col].replace(95, np.nan, inplace=True)\n",
    "mother_edu = data[high_grade_moth_cols].max(axis=1)\n",
    "\n",
    "data['mother_dropout'] = (mother_edu < 12).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[mother_edu.isna(), 'mother_dropout'] = np.nan\n",
    "data['mother_highschool'] = (mother_edu == 12).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[mother_edu.isna(), 'mother_highschool'] = np.nan\n",
    "data['mother_somecollege'] = (mother_edu >= 13).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[mother_edu.isna(), 'mother_somecollege'] = np.nan\n",
    "\n",
    "# maternal AFQT score\n",
    "data['age_adjusted_AFQT'] = data['AFQT_Pct81_REV']\n",
    "age_adjustment_factors = {\n",
    "    14: 35.60881 / 28.79544,\n",
    "    15: 35.60881 / 32.86273,\n",
    "    16: 35.60881 / 32.86273,\n",
    "    17: 35.60881 / 36.3544,\n",
    "    18: 35.60881 / 33.45777,\n",
    "    19: 35.60881 / 36.84,\n",
    "    20: 35.60881 / 41.84536,\n",
    "    21: 35.60881 / 40.95177,\n",
    "    22: 35.60881 / 42.82069\n",
    "}\n",
    "for age, factor in age_adjustment_factors.items():\n",
    "    data.loc[data['Age_Mom79'] == age, 'age_adjusted_AFQT'] *= factor\n",
    "data['age_adjusted_AFQT_std'] = (data['age_adjusted_AFQT'] - data['age_adjusted_AFQT'].mean()) / data['age_adjusted_AFQT'].std()\n",
    "\n",
    "# Impute missing values for AgeAFQT_std\n",
    "data['impAFQT_std'] = data['age_adjusted_AFQT_std']\n",
    "conditional_means = data.groupby(['Black', 'Hispanic', 'Age_Moth_Birth'])['age_adjusted_AFQT_std'].transform('mean')\n",
    "data['impAFQT_std'] = data['impAFQT_std'].fillna(conditional_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>White/Hispanic, Head Start</th>\n",
       "      <th>White/Hispanic, Preschool</th>\n",
       "      <th>White/Hispanic, None</th>\n",
       "      <th>Black, Head Start</th>\n",
       "      <th>Black, Preschool</th>\n",
       "      <th>Black, None</th>\n",
       "      <th>Head startâ€”none diff. (in SD units), White/Hispanic</th>\n",
       "      <th>Head startâ€”none diff. (in SD units), Black</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>permanent_family_income</th>\n",
       "      <td>26388.322266</td>\n",
       "      <td>50042.378906</td>\n",
       "      <td>35153.972656</td>\n",
       "      <td>22788.820312</td>\n",
       "      <td>32404.78125</td>\n",
       "      <td>25210.658203</td>\n",
       "      <td>-0.28597</td>\n",
       "      <td>-0.114621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permanent_family_income std</th>\n",
       "      <td>19458.916016</td>\n",
       "      <td>45940.242188</td>\n",
       "      <td>23423.578125</td>\n",
       "      <td>14835.428711</td>\n",
       "      <td>26156.855469</td>\n",
       "      <td>21755.886719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permanent_family_incomeFixed effects subsample</th>\n",
       "      <td>26575.185547</td>\n",
       "      <td>45532.839844</td>\n",
       "      <td>36482.328125</td>\n",
       "      <td>23876.337891</td>\n",
       "      <td>30637.169922</td>\n",
       "      <td>23698.416016</td>\n",
       "      <td>-0.397885</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>permanent_family_incomeFixed effects subsample std</th>\n",
       "      <td>21131.683594</td>\n",
       "      <td>25011.263672</td>\n",
       "      <td>24515.021484</td>\n",
       "      <td>16325.375977</td>\n",
       "      <td>26975.996094</td>\n",
       "      <td>18691.693359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_dropout</th>\n",
       "      <td>0.522565</td>\n",
       "      <td>0.197659</td>\n",
       "      <td>0.437905</td>\n",
       "      <td>0.362222</td>\n",
       "      <td>0.226054</td>\n",
       "      <td>0.399354</td>\n",
       "      <td>0.173281</td>\n",
       "      <td>-0.077261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_dropout std</th>\n",
       "      <td>0.500085</td>\n",
       "      <td>0.398493</td>\n",
       "      <td>0.496244</td>\n",
       "      <td>0.481178</td>\n",
       "      <td>0.419078</td>\n",
       "      <td>0.490029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_dropoutFixed effects subsample</th>\n",
       "      <td>0.498403</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.378587</td>\n",
       "      <td>0.400651</td>\n",
       "      <td>0.256281</td>\n",
       "      <td>0.382524</td>\n",
       "      <td>0.250569</td>\n",
       "      <td>0.03767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_dropoutFixed effects subsample std</th>\n",
       "      <td>0.500798</td>\n",
       "      <td>0.410745</td>\n",
       "      <td>0.485303</td>\n",
       "      <td>0.490831</td>\n",
       "      <td>0.43768</td>\n",
       "      <td>0.486476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_somecollege</th>\n",
       "      <td>0.218527</td>\n",
       "      <td>0.390117</td>\n",
       "      <td>0.225209</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.286329</td>\n",
       "      <td>-0.015187</td>\n",
       "      <td>0.005493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_somecollege std</th>\n",
       "      <td>0.413739</td>\n",
       "      <td>0.488094</td>\n",
       "      <td>0.417816</td>\n",
       "      <td>0.453751</td>\n",
       "      <td>0.500663</td>\n",
       "      <td>0.452289</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_somecollegeFixed effects subsample</th>\n",
       "      <td>0.188498</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.263797</td>\n",
       "      <td>0.289902</td>\n",
       "      <td>0.422111</td>\n",
       "      <td>0.308738</td>\n",
       "      <td>-0.16771</td>\n",
       "      <td>-0.04019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mother_somecollegeFixed effects subsample std</th>\n",
       "      <td>0.391736</td>\n",
       "      <td>0.482575</td>\n",
       "      <td>0.440934</td>\n",
       "      <td>0.454458</td>\n",
       "      <td>0.495142</td>\n",
       "      <td>0.462422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_adjusted_AFQT_std</th>\n",
       "      <td>-0.464037</td>\n",
       "      <td>0.163425</td>\n",
       "      <td>-0.251719</td>\n",
       "      <td>-0.760602</td>\n",
       "      <td>-0.512431</td>\n",
       "      <td>-0.681845</td>\n",
       "      <td>-0.246892</td>\n",
       "      <td>-0.129807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_adjusted_AFQT_std std</th>\n",
       "      <td>0.724694</td>\n",
       "      <td>0.851276</td>\n",
       "      <td>0.850553</td>\n",
       "      <td>0.49704</td>\n",
       "      <td>0.713849</td>\n",
       "      <td>0.613654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_adjusted_AFQT_stdFixed effects subsample</th>\n",
       "      <td>-0.466045</td>\n",
       "      <td>0.080349</td>\n",
       "      <td>-0.215757</td>\n",
       "      <td>-0.778181</td>\n",
       "      <td>-0.577228</td>\n",
       "      <td>-0.714287</td>\n",
       "      <td>-0.302698</td>\n",
       "      <td>-0.108639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_adjusted_AFQT_stdFixed effects subsample std</th>\n",
       "      <td>0.680689</td>\n",
       "      <td>0.860089</td>\n",
       "      <td>0.815219</td>\n",
       "      <td>0.495305</td>\n",
       "      <td>0.690932</td>\n",
       "      <td>0.589313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HighGrade_GMom79</th>\n",
       "      <td>8.476923</td>\n",
       "      <td>10.571823</td>\n",
       "      <td>9.231972</td>\n",
       "      <td>9.684338</td>\n",
       "      <td>10.803348</td>\n",
       "      <td>9.990373</td>\n",
       "      <td>-0.217998</td>\n",
       "      <td>-0.112916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HighGrade_GMom79 std</th>\n",
       "      <td>3.48751</td>\n",
       "      <td>2.939516</td>\n",
       "      <td>3.536411</td>\n",
       "      <td>2.560215</td>\n",
       "      <td>2.655074</td>\n",
       "      <td>2.75854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HighGrade_GMom79Fixed effects subsample</th>\n",
       "      <td>8.539792</td>\n",
       "      <td>10.431966</td>\n",
       "      <td>9.494585</td>\n",
       "      <td>9.79078</td>\n",
       "      <td>10.413408</td>\n",
       "      <td>10.173245</td>\n",
       "      <td>-0.279739</td>\n",
       "      <td>-0.147793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HighGrade_GMom79Fixed effects subsample std</th>\n",
       "      <td>3.447922</td>\n",
       "      <td>3.101322</td>\n",
       "      <td>3.455002</td>\n",
       "      <td>2.566313</td>\n",
       "      <td>2.716377</td>\n",
       "      <td>2.53473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample size</th>\n",
       "      <td>421</td>\n",
       "      <td>769</td>\n",
       "      <td>2167</td>\n",
       "      <td>450</td>\n",
       "      <td>261</td>\n",
       "      <td>929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fixed effects subsample size</th>\n",
       "      <td>313</td>\n",
       "      <td>490</td>\n",
       "      <td>906</td>\n",
       "      <td>307</td>\n",
       "      <td>199</td>\n",
       "      <td>515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   White/Hispanic, Head Start  \\\n",
       "permanent_family_income                                          26388.322266   \n",
       "permanent_family_income std                                      19458.916016   \n",
       "permanent_family_incomeFixed effects subsample                   26575.185547   \n",
       "permanent_family_incomeFixed effects subsample std               21131.683594   \n",
       "mother_dropout                                                       0.522565   \n",
       "mother_dropout std                                                   0.500085   \n",
       "mother_dropoutFixed effects subsample                                0.498403   \n",
       "mother_dropoutFixed effects subsample std                            0.500798   \n",
       "mother_somecollege                                                   0.218527   \n",
       "mother_somecollege std                                               0.413739   \n",
       "mother_somecollegeFixed effects subsample                            0.188498   \n",
       "mother_somecollegeFixed effects subsample std                        0.391736   \n",
       "age_adjusted_AFQT_std                                               -0.464037   \n",
       "age_adjusted_AFQT_std std                                            0.724694   \n",
       "age_adjusted_AFQT_stdFixed effects subsample                        -0.466045   \n",
       "age_adjusted_AFQT_stdFixed effects subsample std                     0.680689   \n",
       "HighGrade_GMom79                                                     8.476923   \n",
       "HighGrade_GMom79 std                                                  3.48751   \n",
       "HighGrade_GMom79Fixed effects subsample                              8.539792   \n",
       "HighGrade_GMom79Fixed effects subsample std                          3.447922   \n",
       "Sample size                                                               421   \n",
       "Fixed effects subsample size                                              313   \n",
       "\n",
       "                                                   White/Hispanic, Preschool  \\\n",
       "permanent_family_income                                         50042.378906   \n",
       "permanent_family_income std                                     45940.242188   \n",
       "permanent_family_incomeFixed effects subsample                  45532.839844   \n",
       "permanent_family_incomeFixed effects subsample std              25011.263672   \n",
       "mother_dropout                                                      0.197659   \n",
       "mother_dropout std                                                  0.398493   \n",
       "mother_dropoutFixed effects subsample                               0.214286   \n",
       "mother_dropoutFixed effects subsample std                           0.410745   \n",
       "mother_somecollege                                                  0.390117   \n",
       "mother_somecollege std                                              0.488094   \n",
       "mother_somecollegeFixed effects subsample                           0.367347   \n",
       "mother_somecollegeFixed effects subsample std                       0.482575   \n",
       "age_adjusted_AFQT_std                                               0.163425   \n",
       "age_adjusted_AFQT_std std                                           0.851276   \n",
       "age_adjusted_AFQT_stdFixed effects subsample                        0.080349   \n",
       "age_adjusted_AFQT_stdFixed effects subsample std                    0.860089   \n",
       "HighGrade_GMom79                                                   10.571823   \n",
       "HighGrade_GMom79 std                                                2.939516   \n",
       "HighGrade_GMom79Fixed effects subsample                            10.431966   \n",
       "HighGrade_GMom79Fixed effects subsample std                         3.101322   \n",
       "Sample size                                                              769   \n",
       "Fixed effects subsample size                                             490   \n",
       "\n",
       "                                                   White/Hispanic, None  \\\n",
       "permanent_family_income                                    35153.972656   \n",
       "permanent_family_income std                                23423.578125   \n",
       "permanent_family_incomeFixed effects subsample             36482.328125   \n",
       "permanent_family_incomeFixed effects subsample std         24515.021484   \n",
       "mother_dropout                                                 0.437905   \n",
       "mother_dropout std                                             0.496244   \n",
       "mother_dropoutFixed effects subsample                          0.378587   \n",
       "mother_dropoutFixed effects subsample std                      0.485303   \n",
       "mother_somecollege                                             0.225209   \n",
       "mother_somecollege std                                         0.417816   \n",
       "mother_somecollegeFixed effects subsample                      0.263797   \n",
       "mother_somecollegeFixed effects subsample std                  0.440934   \n",
       "age_adjusted_AFQT_std                                         -0.251719   \n",
       "age_adjusted_AFQT_std std                                      0.850553   \n",
       "age_adjusted_AFQT_stdFixed effects subsample                  -0.215757   \n",
       "age_adjusted_AFQT_stdFixed effects subsample std               0.815219   \n",
       "HighGrade_GMom79                                               9.231972   \n",
       "HighGrade_GMom79 std                                           3.536411   \n",
       "HighGrade_GMom79Fixed effects subsample                        9.494585   \n",
       "HighGrade_GMom79Fixed effects subsample std                    3.455002   \n",
       "Sample size                                                        2167   \n",
       "Fixed effects subsample size                                        906   \n",
       "\n",
       "                                                   Black, Head Start  \\\n",
       "permanent_family_income                                 22788.820312   \n",
       "permanent_family_income std                             14835.428711   \n",
       "permanent_family_incomeFixed effects subsample          23876.337891   \n",
       "permanent_family_incomeFixed effects subsample std      16325.375977   \n",
       "mother_dropout                                              0.362222   \n",
       "mother_dropout std                                          0.481178   \n",
       "mother_dropoutFixed effects subsample                       0.400651   \n",
       "mother_dropoutFixed effects subsample std                   0.490831   \n",
       "mother_somecollege                                          0.288889   \n",
       "mother_somecollege std                                      0.453751   \n",
       "mother_somecollegeFixed effects subsample                   0.289902   \n",
       "mother_somecollegeFixed effects subsample std               0.454458   \n",
       "age_adjusted_AFQT_std                                      -0.760602   \n",
       "age_adjusted_AFQT_std std                                    0.49704   \n",
       "age_adjusted_AFQT_stdFixed effects subsample               -0.778181   \n",
       "age_adjusted_AFQT_stdFixed effects subsample std            0.495305   \n",
       "HighGrade_GMom79                                            9.684338   \n",
       "HighGrade_GMom79 std                                        2.560215   \n",
       "HighGrade_GMom79Fixed effects subsample                      9.79078   \n",
       "HighGrade_GMom79Fixed effects subsample std                 2.566313   \n",
       "Sample size                                                      450   \n",
       "Fixed effects subsample size                                     307   \n",
       "\n",
       "                                                   Black, Preschool  \\\n",
       "permanent_family_income                                 32404.78125   \n",
       "permanent_family_income std                            26156.855469   \n",
       "permanent_family_incomeFixed effects subsample         30637.169922   \n",
       "permanent_family_incomeFixed effects subsample std     26975.996094   \n",
       "mother_dropout                                             0.226054   \n",
       "mother_dropout std                                         0.419078   \n",
       "mother_dropoutFixed effects subsample                      0.256281   \n",
       "mother_dropoutFixed effects subsample std                   0.43768   \n",
       "mother_somecollege                                         0.482759   \n",
       "mother_somecollege std                                     0.500663   \n",
       "mother_somecollegeFixed effects subsample                  0.422111   \n",
       "mother_somecollegeFixed effects subsample std              0.495142   \n",
       "age_adjusted_AFQT_std                                     -0.512431   \n",
       "age_adjusted_AFQT_std std                                  0.713849   \n",
       "age_adjusted_AFQT_stdFixed effects subsample              -0.577228   \n",
       "age_adjusted_AFQT_stdFixed effects subsample std           0.690932   \n",
       "HighGrade_GMom79                                          10.803348   \n",
       "HighGrade_GMom79 std                                       2.655074   \n",
       "HighGrade_GMom79Fixed effects subsample                   10.413408   \n",
       "HighGrade_GMom79Fixed effects subsample std                2.716377   \n",
       "Sample size                                                     261   \n",
       "Fixed effects subsample size                                    199   \n",
       "\n",
       "                                                     Black, None  \\\n",
       "permanent_family_income                             25210.658203   \n",
       "permanent_family_income std                         21755.886719   \n",
       "permanent_family_incomeFixed effects subsample      23698.416016   \n",
       "permanent_family_incomeFixed effects subsample std  18691.693359   \n",
       "mother_dropout                                          0.399354   \n",
       "mother_dropout std                                      0.490029   \n",
       "mother_dropoutFixed effects subsample                   0.382524   \n",
       "mother_dropoutFixed effects subsample std               0.486476   \n",
       "mother_somecollege                                      0.286329   \n",
       "mother_somecollege std                                  0.452289   \n",
       "mother_somecollegeFixed effects subsample               0.308738   \n",
       "mother_somecollegeFixed effects subsample std           0.462422   \n",
       "age_adjusted_AFQT_std                                  -0.681845   \n",
       "age_adjusted_AFQT_std std                               0.613654   \n",
       "age_adjusted_AFQT_stdFixed effects subsample           -0.714287   \n",
       "age_adjusted_AFQT_stdFixed effects subsample std        0.589313   \n",
       "HighGrade_GMom79                                        9.990373   \n",
       "HighGrade_GMom79 std                                     2.75854   \n",
       "HighGrade_GMom79Fixed effects subsample                10.173245   \n",
       "HighGrade_GMom79Fixed effects subsample std              2.53473   \n",
       "Sample size                                                  929   \n",
       "Fixed effects subsample size                                 515   \n",
       "\n",
       "                                                   Head startâ€”none diff. (in SD units), White/Hispanic  \\\n",
       "permanent_family_income                                                                      -0.28597    \n",
       "permanent_family_income std                                                                       NaN    \n",
       "permanent_family_incomeFixed effects subsample                                              -0.397885    \n",
       "permanent_family_incomeFixed effects subsample std                                                NaN    \n",
       "mother_dropout                                                                               0.173281    \n",
       "mother_dropout std                                                                                NaN    \n",
       "mother_dropoutFixed effects subsample                                                        0.250569    \n",
       "mother_dropoutFixed effects subsample std                                                         NaN    \n",
       "mother_somecollege                                                                          -0.015187    \n",
       "mother_somecollege std                                                                            NaN    \n",
       "mother_somecollegeFixed effects subsample                                                    -0.16771    \n",
       "mother_somecollegeFixed effects subsample std                                                     NaN    \n",
       "age_adjusted_AFQT_std                                                                       -0.246892    \n",
       "age_adjusted_AFQT_std std                                                                         NaN    \n",
       "age_adjusted_AFQT_stdFixed effects subsample                                                -0.302698    \n",
       "age_adjusted_AFQT_stdFixed effects subsample std                                                  NaN    \n",
       "HighGrade_GMom79                                                                            -0.217998    \n",
       "HighGrade_GMom79 std                                                                              NaN    \n",
       "HighGrade_GMom79Fixed effects subsample                                                     -0.279739    \n",
       "HighGrade_GMom79Fixed effects subsample std                                                       NaN    \n",
       "Sample size                                                                                       NaN    \n",
       "Fixed effects subsample size                                                                      NaN    \n",
       "\n",
       "                                                   Head startâ€”none diff. (in SD units), Black  \n",
       "permanent_family_income                                                             -0.114621  \n",
       "permanent_family_income std                                                               NaN  \n",
       "permanent_family_incomeFixed effects subsample                                       0.008844  \n",
       "permanent_family_incomeFixed effects subsample std                                        NaN  \n",
       "mother_dropout                                                                      -0.077261  \n",
       "mother_dropout std                                                                        NaN  \n",
       "mother_dropoutFixed effects subsample                                                 0.03767  \n",
       "mother_dropoutFixed effects subsample std                                                 NaN  \n",
       "mother_somecollege                                                                   0.005493  \n",
       "mother_somecollege std                                                                    NaN  \n",
       "mother_somecollegeFixed effects subsample                                            -0.04019  \n",
       "mother_somecollegeFixed effects subsample std                                             NaN  \n",
       "age_adjusted_AFQT_std                                                               -0.129807  \n",
       "age_adjusted_AFQT_std std                                                                 NaN  \n",
       "age_adjusted_AFQT_stdFixed effects subsample                                        -0.108639  \n",
       "age_adjusted_AFQT_stdFixed effects subsample std                                          NaN  \n",
       "HighGrade_GMom79                                                                    -0.112916  \n",
       "HighGrade_GMom79 std                                                                      NaN  \n",
       "HighGrade_GMom79Fixed effects subsample                                             -0.147793  \n",
       "HighGrade_GMom79Fixed effects subsample std                                               NaN  \n",
       "Sample size                                                                               NaN  \n",
       "Fixed effects subsample size                                                              NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table 1\n",
    "covariates_to_list = [\"permanent_family_income\", \"mother_dropout\", \"mother_somecollege\", \"age_adjusted_AFQT_std\", \"HighGrade_GMom79\"]\n",
    "column_names = [\"White/Hispanic, Head Start\", \"White/Hispanic, Preschool\", \"White/Hispanic, None\",\n",
    "                \"Black, Head Start\", \"Black, Preschool\", \"Black, None\",\n",
    "                \"Head startâ€”none diff. (in SD units), White/Hispanic\", \"Head startâ€”none diff. (in SD units), Black\"]\n",
    "row_names = []\n",
    "for covariate in covariates_to_list:\n",
    "    row_names.append(covariate)\n",
    "    row_names.append(covariate + \" std\")\n",
    "    row_names.append(covariate + \"Fixed effects subsample\")\n",
    "    row_names.append(covariate + \"Fixed effects subsample std\")\n",
    "row_names.append(\"Sample size\")\n",
    "row_names.append(\"Fixed effects subsample size\")\n",
    "table1 = pd.DataFrame(index=row_names, columns=column_names)\n",
    "for idx, covariate in enumerate(covariates_to_list):\n",
    "    mask = (data['NonBlack'] == 1) & (data['program_participation_type_90'] == 1)\n",
    "    table1.loc[covariate, \"White/Hispanic, Head Start\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \" std\", \"White/Hispanic, Head Start\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Sample size\", \"White/Hispanic, Head Start\"] = mask.sum()\n",
    "    mask = (data['NonBlack'] == 1) & (data['program_participation_type_90'] == 2)\n",
    "    table1.loc[covariate, \"White/Hispanic, Preschool\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \" std\", \"White/Hispanic, Preschool\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Sample size\", \"White/Hispanic, Preschool\"] = mask.sum()\n",
    "    mask = (data['NonBlack'] == 1) & (data['program_participation_type_90'] == 3)\n",
    "    table1.loc[covariate, \"White/Hispanic, None\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \" std\", \"White/Hispanic, None\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Sample size\", \"White/Hispanic, None\"] = mask.sum()\n",
    "    mask = (data['Black'] == 1) & (data['program_participation_type_90'] == 1)\n",
    "    table1.loc[covariate, \"Black, Head Start\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \" std\", \"Black, Head Start\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Sample size\", \"Black, Head Start\"] = mask.sum()\n",
    "    mask = (data['Black'] == 1) & (data['program_participation_type_90'] == 2)\n",
    "    table1.loc[covariate, \"Black, Preschool\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \" std\", \"Black, Preschool\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Sample size\", \"Black, Preschool\"] = mask.sum()\n",
    "    mask = (data['Black'] == 1) & (data['program_participation_type_90'] == 3)\n",
    "    table1.loc[covariate, \"Black, None\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \" std\", \"Black, None\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Sample size\", \"Black, None\"] = mask.sum()\n",
    "\n",
    "    mask_all = (data['NonBlack'] == 1) & (data['program_participation_type_90'].notna())\n",
    "    mask_hs = (data['NonBlack'] == 1) & (data['program_participation_type_90'] == 1)\n",
    "    mask_none = (data['NonBlack'] == 1) & (data['program_participation_type_90'] == 3)\n",
    "    table1.loc[covariate, \"Head startâ€”none diff. (in SD units), White/Hispanic\"] = (data.loc[mask_hs, covariate].mean() - data.loc[mask_none, covariate].mean()) / data.loc[mask_all, covariate].std()\n",
    "    table1.loc[covariate + \" std\", \"Head startâ€”none diff. (in SD units), White/Hispanic\"] = np.nan\n",
    "\n",
    "    mask_all = (data['Black'] == 1) & (data['program_participation_type_90'].notna())\n",
    "    mask_hs = (data['Black'] == 1) & (data['program_participation_type_90'] == 1)\n",
    "    mask_none = (data['Black'] == 1) & (data['program_participation_type_90'] == 3)\n",
    "    table1.loc[covariate, \"Head startâ€”none diff. (in SD units), Black\"] = (data.loc[mask_hs, covariate].mean() - data.loc[mask_none, covariate].mean()) / data.loc[mask_all, covariate].std()\n",
    "    table1.loc[covariate + \" std\", \"Head startâ€”none diff. (in SD units), Black\"] = np.nan\n",
    "\n",
    "\n",
    "    mask = (data['NonBlack'] == 1) & (data['program_participation_fixedeffect_type_90'] == 1)\n",
    "    table1.loc[covariate + \"Fixed effects subsample\", \"White/Hispanic, Head Start\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \"Fixed effects subsample std\", \"White/Hispanic, Head Start\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Fixed effects subsample size\", \"White/Hispanic, Head Start\"] = mask.sum()\n",
    "    mask = (data['NonBlack'] == 1) & (data['program_participation_fixedeffect_type_90'] == 2)\n",
    "    table1.loc[covariate + \"Fixed effects subsample\", \"White/Hispanic, Preschool\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \"Fixed effects subsample std\", \"White/Hispanic, Preschool\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Fixed effects subsample size\", \"White/Hispanic, Preschool\"] = mask.sum()\n",
    "    mask = (data['NonBlack'] == 1) & (data['program_participation_fixedeffect_type_90'] == 3)\n",
    "    table1.loc[covariate + \"Fixed effects subsample\", \"White/Hispanic, None\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \"Fixed effects subsample std\", \"White/Hispanic, None\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Fixed effects subsample size\", \"White/Hispanic, None\"] = mask.sum()\n",
    "    mask = (data['Black'] == 1) & (data['program_participation_fixedeffect_type_90'] == 1)\n",
    "    table1.loc[covariate + \"Fixed effects subsample\", \"Black, Head Start\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \"Fixed effects subsample std\", \"Black, Head Start\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Fixed effects subsample size\", \"Black, Head Start\"] = mask.sum()\n",
    "    mask = (data['Black'] == 1) & (data['program_participation_fixedeffect_type_90'] == 2)\n",
    "    table1.loc[covariate + \"Fixed effects subsample\", \"Black, Preschool\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \"Fixed effects subsample std\", \"Black, Preschool\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Fixed effects subsample size\", \"Black, Preschool\"] = mask.sum()\n",
    "    mask = (data['Black'] == 1) & (data['program_participation_fixedeffect_type_90'] == 3)\n",
    "    table1.loc[covariate + \"Fixed effects subsample\", \"Black, None\"] = data.loc[mask, covariate].mean()\n",
    "    table1.loc[covariate + \"Fixed effects subsample std\", \"Black, None\"] = data.loc[mask, covariate].std()\n",
    "    table1.loc[\"Fixed effects subsample size\", \"Black, None\"] = mask.sum()\n",
    "\n",
    "    mask_all = (data['NonBlack'] == 1) & (data['program_participation_fixedeffect_type_90'].notna())\n",
    "    mask_hs = (data['NonBlack'] == 1) & (data['program_participation_fixedeffect_type_90'] == 1)\n",
    "    mask_none = (data['NonBlack'] == 1) & (data['program_participation_fixedeffect_type_90'] == 3)\n",
    "    table1.loc[covariate + \"Fixed effects subsample\", \"Head startâ€”none diff. (in SD units), White/Hispanic\"] = (data.loc[mask_hs, covariate].mean() - data.loc[mask_none, covariate].mean()) / data.loc[mask_all, covariate].std()\n",
    "    table1.loc[covariate + \"Fixed effects subsample std\", \"Head startâ€”none diff. (in SD units), White/Hispanic\"] = np.nan\n",
    "\n",
    "    mask_all = (data['Black'] == 1) & (data['program_participation_fixedeffect_type_90'].notna())\n",
    "    mask_hs = (data['Black'] == 1) & (data['program_participation_fixedeffect_type_90'] == 1)\n",
    "    mask_none = (data['Black'] == 1) & (data['program_participation_fixedeffect_type_90'] == 3)\n",
    "    table1.loc[covariate + \"Fixed effects subsample\", \"Head startâ€”none diff. (in SD units), Black\"] = (data.loc[mask_hs, covariate].mean() - data.loc[mask_none, covariate].mean()) / data.loc[mask_all, covariate].std()\n",
    "    table1.loc[covariate + \"Fixed effects subsample std\", \"Head startâ€”none diff. (in SD units), Black\"] = np.nan\n",
    "\n",
    "table1.to_csv(\"table1.csv\")\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{92}'] = data[f'live_with_mother_{91}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{93}'] = data[f'live_with_mother_{91}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{78}'] = data[f'live_with_mother_{79}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{77}'] = data[f'live_with_mother_{79}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'live_with_mother_{76}'] = data[f'live_with_mother_{79}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"reside_with_mother_0to3\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Ear86\"] = data[\"Ear88\"]\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Blood86\"] = data[\"Blood88\"]\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Epilepsy86\"] = data[\"Epilepsy88\"]\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'temp{condition}{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{condition}_before'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['prexisting_health_conditions'] = data[[f'{condition}_before' for condition in health_conditions]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:52: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['low_birth_weight'] = (data['BirthWeight'] < 88).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['very_low_birth_weight'] = (data['BirthWeight'] < 53).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Attrit'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:66: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Sample90\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:73: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Three'] = ((data['Age_1stHS88'] <= 3) | (data['Age_1stHS90'] <= 3)).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['NotThree'] = ((data['Age_1stHS88'] > 3) & (data['Age_1stHS88'].notna()) | (data['Age_1stHS90'] > 3) & (data['Age_1stHS90'].notna())).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['HS_3'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:78: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['HS_3'].fillna(0, inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['program_participation_fixedeffect_type_90_before3'] = data['program_participation_fixedeffect_type_90']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['income_0to3'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data['NetFamInc90']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc89', 'NetFamInc90']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc88', 'NetFamInc89', 'NetFamInc90']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:95: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc87', 'NetFamInc88', 'NetFamInc89', 'NetFamInc90']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:97: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc86', 'NetFamInc87', 'NetFamInc88', 'NetFamInc89']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:99: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc85', 'NetFamInc86', 'NetFamInc87', 'NetFamInc88']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:101: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc84', 'NetFamInc85', 'NetFamInc86', 'NetFamInc87']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc83', 'NetFamInc84', 'NetFamInc85', 'NetFamInc86']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:105: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc82', 'NetFamInc83', 'NetFamInc84', 'NetFamInc85']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc81', 'NetFamInc82', 'NetFamInc83', 'NetFamInc84']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc80', 'NetFamInc81', 'NetFamInc82', 'NetFamInc83']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:111: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc79', 'NetFamInc80', 'NetFamInc81', 'NetFamInc82']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc79', 'NetFamInc80', 'NetFamInc81']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc78', 'NetFamInc79', 'NetFamInc80']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:117: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data[['NetFamInc78', 'NetFamInc79']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:119: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[temp_col] = data['NetFamInc78']\n",
      "/Users/zekai.wang/anaconda3/envs/stat256/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:123: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['log_income_0to3'] = np.log(data['income_0to3'])\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:124: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['log_income_0to3'].replace(-np.inf, np.nan, inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:126: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['income_at_3'] = np.nan\n",
      "/Users/zekai.wang/anaconda3/envs/stat256/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:129: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['log_income_at_3'] = np.log(data['income_at_3'])\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:130: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['log_income_at_3'].replace(-np.inf, np.nan, inplace=True)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:134: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['first_born'] = (data['BirthOrder']==1).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:137: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['male'] = (data['Sex_Child']==1).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:140: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PPVTat3'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:149: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['HOME_Pct_0to3'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:158: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Moth_HrsWorked_BefBirth'] = data[[f'Moth_HrsWorked_{qtr}_Qtr_Before' for qtr in range(1, 5)]].mean(axis=1) / 13\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:159: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Moth_HrsWorked_0to3'] = data[[f'Moth_HrsWorked_{qtr}_Qtr' for qtr in range(1, 13)]].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:160: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Moth_HrsWorked_Avg_0to3'] = data[[f'Moth_HrsWorked_{qtr}_Avg' for qtr in range(1, 13)]].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:161: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Moth_HrsWorked_0to1'] = data[[f'Moth_HrsWorked_{qtr}_Avg' for qtr in range(1, 5)]].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:164: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Father_HH_0to3'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:178: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'GMom{year}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['GMom_0to3'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'RelCare_{y}_Yr'] = np.where((data[f'ChildCare_Type_{y}_Yr'].notna()) & (data[f'ChildCare_Type_{y}_Yr'] <= 10), 1, np.nan)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'NonRelCare_{y}_Yr'] = np.where((data[f'ChildCare_Type_{y}_Yr'].notna()) & (data[f'ChildCare_Type_{y}_Yr'] > 10), 1, np.nan)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:223: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MomCare_{y}_Yr'] = np.where((data[f'RelCare_{y}_Yr'] == 0) & (data[f'NonRelCare_{y}_Yr'] == 0), 1, np.nan)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'RelCare_{y}_Yr'] = np.where((data[f'ChildCare_Type_{y}_Yr'].notna()) & (data[f'ChildCare_Type_{y}_Yr'] <= 10), 1, np.nan)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'NonRelCare_{y}_Yr'] = np.where((data[f'ChildCare_Type_{y}_Yr'].notna()) & (data[f'ChildCare_Type_{y}_Yr'] > 10), 1, np.nan)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:223: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MomCare_{y}_Yr'] = np.where((data[f'RelCare_{y}_Yr'] == 0) & (data[f'NonRelCare_{y}_Yr'] == 0), 1, np.nan)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:217: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'RelCare_{y}_Yr'] = np.where((data[f'ChildCare_Type_{y}_Yr'].notna()) & (data[f'ChildCare_Type_{y}_Yr'] <= 10), 1, np.nan)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:220: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'NonRelCare_{y}_Yr'] = np.where((data[f'ChildCare_Type_{y}_Yr'].notna()) & (data[f'ChildCare_Type_{y}_Yr'] > 10), 1, np.nan)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:223: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'MomCare_{y}_Yr'] = np.where((data[f'RelCare_{y}_Yr'] == 0) & (data[f'NonRelCare_{y}_Yr'] == 0), 1, np.nan)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:226: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['RelCare'] = data[[f'RelCare_{y}_Yr' for y in range(1, 4)]].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:227: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['NonRelCare'] = data[[f'NonRelCare_{y}_Yr' for y in range(1, 4)]].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:228: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['MomCare'] = data[[f'MomCare_{y}_Yr' for y in range(1, 4)]].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:232: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Alc_BefBirth'] = (data['Freq_Alc_BefBirth'] >= 3).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:236: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{x}_temp'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:236: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{x}_temp'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:243: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Doctor_0to3'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:248: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Dentist_0to3'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:260: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Premature'] = (data['BornEarlyorLate'] == 1).apply(lambda x: x if np.isnan(x) else int(x))\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{x}_0to3'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:265: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'{x}_0to3'] = np.nan\n",
      "/Users/zekai.wang/anaconda3/envs/stat256/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3303954619.py:274: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['logBW'] = np.log(data['BirthWeight'])\n"
     ]
    }
   ],
   "source": [
    "# More covariates\n",
    "\n",
    "# live in the same household as mother, 0-3 years old\n",
    "for year in range(79, 92):\n",
    "    data[f'live_with_mother_{year}'] = np.nan\n",
    "    mask = data[f'Res{year}'] == 1\n",
    "    data.loc[mask, f'live_with_mother_{year}'] = 1\n",
    "    mask = (data[f'Res{year}'] != 1) & (data[f'Res{year}'].notna())\n",
    "    data.loc[mask, f'live_with_mother_{year}'] = 0\n",
    "\n",
    "data[f'live_with_mother_{92}'] = data[f'live_with_mother_{91}']\n",
    "data[f'live_with_mother_{93}'] = data[f'live_with_mother_{91}']\n",
    "data[f'live_with_mother_{78}'] = data[f'live_with_mother_{79}']\n",
    "data[f'live_with_mother_{77}'] = data[f'live_with_mother_{79}']\n",
    "data[f'live_with_mother_{76}'] = data[f'live_with_mother_{79}']\n",
    "\n",
    "data[\"reside_with_mother_0to3\"] = np.nan\n",
    "for year in range(76, 91):\n",
    "    mask = (104 - data[f'cleaned_age_yr_104'] == year)\n",
    "    live_mask = (data[f'live_with_mother_{year}'] == 1) & (data[f'live_with_mother_{year+1}'] <= 1) & (data[f'live_with_mother_{year+2}'] <= 1) & (data[f'live_with_mother_{year+3}'] <= 1)\n",
    "    missing_mask = (data[f'live_with_mother_{year}'].isna()) & (data[f'live_with_mother_{year+1}'].isna()) & (data[f'live_with_mother_{year+2}'].isna()) & (data[f'live_with_mother_{year+3}'].isna())\n",
    "    data.loc[mask & live_mask, \"reside_with_mother_0to3\"] = 1\n",
    "    data.loc[mask & ~live_mask, \"reside_with_mother_0to3\"] = 0\n",
    "    data.loc[mask & missing_mask, \"reside_with_mother_0to3\"] = np.nan\n",
    "\n",
    "for year in range(76, 94):\n",
    "    del data[f'live_with_mother_{year}']\n",
    "\n",
    "# Preexisting health conditions prior to age 5\n",
    "data[\"Ear86\"] = data[\"Ear88\"]\n",
    "data[\"Blood86\"] = data[\"Blood88\"]\n",
    "data[\"Epilepsy86\"] = data[\"Epilepsy88\"]\n",
    "health_conditions = [\"Brain\", \"Hyper\", \"Asthma\", \"Resp\", \"Speech\", \"Deaf\", \"Blind\", \"Disturb\", \"Allergy\", \"Crippled\", \"Retard\", \"Heart\", \"Nerve\", \"Ear\", \"Blood\", \"Epilepsy\", \"OtherLim\"]\n",
    "for condition in health_conditions:\n",
    "    for year in range(86, 92, 2):\n",
    "        data[f'temp{condition}{year}'] = np.nan\n",
    "        mask = data[f'{condition}{year}'] > 0\n",
    "        data.loc[mask, f'temp{condition}{year}'] = 1\n",
    "    data[condition] = data[[f'temp{condition}{year}' for year in range(86, 92, 2)]].max(axis=1)\n",
    "    data[f'{condition}_before'] = np.nan\n",
    "    for year in range(86, 92, 2):\n",
    "        mask = (data[f'temp{condition}{year}'] == 1) & (data[f'cleaned_age_mo_{year}'] < 60)\n",
    "        data.loc[mask, f'{condition}_before'] = 1\n",
    "    data.drop(columns=[f'temp{condition}{year}' for year in range(86, 92, 2)], inplace=True)\n",
    "\n",
    "data['prexisting_health_conditions'] = data[[f'{condition}_before' for condition in health_conditions]].max(axis=1)\n",
    "data.loc[data[\"reside_with_mother_0to3\"].isna(), 'prexisting_health_conditions'] = np.nan\n",
    "data.loc[(~data[\"reside_with_mother_0to3\"].isna()) & data['prexisting_health_conditions'].isna(), 'prexisting_health_conditions'] = 0 # TODO: ad hoc\n",
    "\n",
    "\n",
    "# low birth weight\n",
    "data['low_birth_weight'] = (data['BirthWeight'] < 88).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[data['BirthWeight'].isna(), 'low_birth_weight'] = np.nan\n",
    "\n",
    "data['very_low_birth_weight'] = (data['BirthWeight'] < 53).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[data['BirthWeight'].isna(), 'very_low_birth_weight'] = np.nan\n",
    "\n",
    "# attrition: if respondent disappears from sample before age 19\n",
    "data['Attrit'] = np.nan\n",
    "data.loc[data['YA_LastInterview'] == 2004, 'Attrit'] = 0\n",
    "data.loc[(data['YA_LastInterview'] != 2004) & data['YA_LastInterview'].notna(), 'Attrit'] = 1\n",
    "for year in [94, 96, 98, 100, 102]:\n",
    "    data.loc[(data['Attrit'] == 1) & (data['YA_LastInterview'] == 1900 + year) & (data[f'cleaned_age_yr_{year}'] >= 19), 'Attrit'] = 0\n",
    "\n",
    "# some dummy for estimation\n",
    "data[\"Sample90\"] = np.nan\n",
    "mask = (data['eligibility_siblingdifferenttreatment_90'] == 1) & (data['Attrit'] == 0) &\\\n",
    "      (((data['SampleID'] != 12) & (data['cleaned_age_yr_104'] >= 19)) | ((data['DOB_Yr_Child']==1985) & (data['DOB_Mo_Child']<8)))\n",
    "data.loc[mask, \"Sample90\"] = 1\n",
    "\n",
    "\n",
    "# Generate the 'HS_3' variable\n",
    "data['Three'] = ((data['Age_1stHS88'] <= 3) | (data['Age_1stHS90'] <= 3)).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data['NotThree'] = ((data['Age_1stHS88'] > 3) & (data['Age_1stHS88'].notna()) | (data['Age_1stHS90'] > 3) & (data['Age_1stHS90'].notna())).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data['HS_3'] = np.nan\n",
    "data.loc[data['Three'] == 1, 'HS_3'] = 1\n",
    "data.loc[data['NotThree'] == 1, 'HS_3'] = 2\n",
    "data['HS_3'].fillna(0, inplace=True)\n",
    "# Separate kids that are in Head Start at age 3 from later\n",
    "data['program_participation_fixedeffect_type_90_before3'] = data['program_participation_fixedeffect_type_90']\n",
    "data.loc[(data['program_participation_fixedeffect_type_90'] == 1) & (data['HS_3'] == 1), 'program_participation_fixedeffect_type_90_before3'] = 0\n",
    "# so 0 is HS before age 3, 1 is HS after age 3, 2 is other pre-school program, 3 is no pre-school program\n",
    "\n",
    "# Log Income Ages 0-3, Log Income at Age 3*\n",
    "data['income_0to3'] = np.nan\n",
    "for age in range(14, 30):\n",
    "    temp_col = f'temp{age}'\n",
    "    if age == 14:\n",
    "        data[temp_col] = data['NetFamInc90']\n",
    "    elif age == 15:\n",
    "        data[temp_col] = data[['NetFamInc89', 'NetFamInc90']].mean(axis=1)\n",
    "    elif age == 16:\n",
    "        data[temp_col] = data[['NetFamInc88', 'NetFamInc89', 'NetFamInc90']].mean(axis=1)\n",
    "    elif age == 17:\n",
    "        data[temp_col] = data[['NetFamInc87', 'NetFamInc88', 'NetFamInc89', 'NetFamInc90']].mean(axis=1)\n",
    "    elif age == 18:\n",
    "        data[temp_col] = data[['NetFamInc86', 'NetFamInc87', 'NetFamInc88', 'NetFamInc89']].mean(axis=1)\n",
    "    elif age == 19:\n",
    "        data[temp_col] = data[['NetFamInc85', 'NetFamInc86', 'NetFamInc87', 'NetFamInc88']].mean(axis=1)\n",
    "    elif age == 20:\n",
    "        data[temp_col] = data[['NetFamInc84', 'NetFamInc85', 'NetFamInc86', 'NetFamInc87']].mean(axis=1)\n",
    "    elif age == 21:\n",
    "        data[temp_col] = data[['NetFamInc83', 'NetFamInc84', 'NetFamInc85', 'NetFamInc86']].mean(axis=1)\n",
    "    elif age == 22:\n",
    "        data[temp_col] = data[['NetFamInc82', 'NetFamInc83', 'NetFamInc84', 'NetFamInc85']].mean(axis=1)\n",
    "    elif age == 23:\n",
    "        data[temp_col] = data[['NetFamInc81', 'NetFamInc82', 'NetFamInc83', 'NetFamInc84']].mean(axis=1)\n",
    "    elif age == 24:\n",
    "        data[temp_col] = data[['NetFamInc80', 'NetFamInc81', 'NetFamInc82', 'NetFamInc83']].mean(axis=1)\n",
    "    elif age == 25:\n",
    "        data[temp_col] = data[['NetFamInc79', 'NetFamInc80', 'NetFamInc81', 'NetFamInc82']].mean(axis=1)\n",
    "    elif age == 26:\n",
    "        data[temp_col] = data[['NetFamInc79', 'NetFamInc80', 'NetFamInc81']].mean(axis=1)\n",
    "    elif age == 27:\n",
    "        data[temp_col] = data[['NetFamInc78', 'NetFamInc79', 'NetFamInc80']].mean(axis=1)\n",
    "    elif age == 28:\n",
    "        data[temp_col] = data[['NetFamInc78', 'NetFamInc79']].mean(axis=1)\n",
    "    elif age == 29:\n",
    "        data[temp_col] = data['NetFamInc78']\n",
    "    data.loc[data['cleaned_age_yr_104'] == age, 'income_0to3'] = data[temp_col]\n",
    "\n",
    "data.drop(columns=[f'temp{age}' for age in range(14, 30)], inplace=True)\n",
    "data['log_income_0to3'] = np.log(data['income_0to3'])\n",
    "data['log_income_0to3'].replace(-np.inf, np.nan, inplace=True)\n",
    "\n",
    "data['income_at_3'] = np.nan\n",
    "for age in range(14, 30):\n",
    "    data.loc[data['cleaned_age_yr_104'] == age, 'income_at_3'] = data[f'NetFamInc{104 - age + 3}']\n",
    "data['log_income_at_3'] = np.log(data['income_at_3'])\n",
    "data['log_income_at_3'].replace(-np.inf, np.nan, inplace=True)\n",
    "\n",
    "\n",
    "# first born\n",
    "data['first_born'] = (data['BirthOrder']==1).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "\n",
    "# male\n",
    "data['male'] = (data['Sex_Child']==1).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "\n",
    "# PPVT score at age 3\n",
    "data['PPVTat3'] = np.nan\n",
    "mask_86 = (data['PPVTAge86'] >= 36) & (data['PPVTAge86'] < 47)\n",
    "data.loc[mask_86, 'PPVTat3'] = data['PPVT_Raw86']\n",
    "mask_88 = (data['PPVTAge88'] >= 36) & (data['PPVTAge88'] < 47) & data['PPVTat3'].isna()\n",
    "data.loc[mask_88, 'PPVTat3'] = data['PPVT_Raw88']\n",
    "mask_90 = (data['PPVTAge90'] >= 36) & (data['PPVTAge90'] < 47) & data['PPVTat3'].isna()\n",
    "data.loc[mask_90, 'PPVTat3'] = data['PPVT_Raw90']\n",
    "\n",
    "# HOME score\n",
    "data['HOME_Pct_0to3'] = np.nan\n",
    "mask_16_19 = (data['cleaned_age_yr_104'] <= 19) & (data['cleaned_age_yr_104'] >= 16)\n",
    "mask_14_15 = (data['cleaned_age_yr_104'] <= 15) & (data['cleaned_age_yr_104'] >= 14)\n",
    "mask_20_21 = (data['cleaned_age_yr_104'] >= 20) & (data['cleaned_age_yr_104'] <= 21)\n",
    "data.loc[mask_16_19, 'HOME_Pct_0to3'] = data[['HOME_Pct86', 'HOME_Pct88']].mean(axis=1)\n",
    "data.loc[mask_14_15, 'HOME_Pct_0to3'] = data[['HOME_Pct88', 'HOME_Pct90']].mean(axis=1)\n",
    "data.loc[mask_20_21, 'HOME_Pct_0to3'] = data['HOME_Pct86']\n",
    "\n",
    "# Mom work\n",
    "data['Moth_HrsWorked_BefBirth'] = data[[f'Moth_HrsWorked_{qtr}_Qtr_Before' for qtr in range(1, 5)]].mean(axis=1) / 13\n",
    "data['Moth_HrsWorked_0to3'] = data[[f'Moth_HrsWorked_{qtr}_Qtr' for qtr in range(1, 13)]].mean(axis=1)\n",
    "data['Moth_HrsWorked_Avg_0to3'] = data[[f'Moth_HrsWorked_{qtr}_Avg' for qtr in range(1, 13)]].mean(axis=1)\n",
    "data['Moth_HrsWorked_0to1'] = data[[f'Moth_HrsWorked_{qtr}_Avg' for qtr in range(1, 5)]].mean(axis=1)\n",
    "\n",
    "# Father present 0-3\n",
    "data['Father_HH_0to3'] = np.nan\n",
    "data.loc[data['cleaned_age_yr_104'] == 14, 'Father_HH_0to3'] = data[['Father_HH90', 'Father_HH92', 'Father_HH93']].mean(axis=1)\n",
    "data.loc[data['cleaned_age_yr_104'] == 15, 'Father_HH_0to3'] = data[['Father_HH89', 'Father_HH90']].mean(axis=1)\n",
    "data.loc[data['cleaned_age_yr_104'] == 16, 'Father_HH_0to3'] = data[['Father_HH88', 'Father_HH89', 'Father_HH90']].mean(axis=1)\n",
    "data.loc[data['cleaned_age_yr_104'] == 17, 'Father_HH_0to3'] = data[['Father_HH87', 'Father_HH88', 'Father_HH89', 'Father_HH90']].mean(axis=1)\n",
    "data.loc[data['cleaned_age_yr_104'] == 18, 'Father_HH_0to3'] = data[['Father_HH86', 'Father_HH87', 'Father_HH88', 'Father_HH89']].mean(axis=1)\n",
    "data.loc[data['cleaned_age_yr_104'] == 19, 'Father_HH_0to3'] = data[['Father_HH85', 'Father_HH86', 'Father_HH87', 'Father_HH88']].mean(axis=1)\n",
    "data.loc[data['cleaned_age_yr_104'] == 20, 'Father_HH_0to3'] = data[['Father_HH84', 'Father_HH85', 'Father_HH86', 'Father_HH87']].mean(axis=1)\n",
    "data.loc[data['cleaned_age_yr_104'] == 21, 'Father_HH_0to3'] = data[['Father_HH84', 'Father_HH85', 'Father_HH86']].mean(axis=1)\n",
    "data.loc[data['cleaned_age_yr_104'] == 22, 'Father_HH_0to3'] = data[['Father_HH84', 'Father_HH85']].mean(axis=1)\n",
    "data.loc[data['cleaned_age_yr_104'] == 23, 'Father_HH_0to3'] = data['Father_HH84']\n",
    "\n",
    "# Grandmother present 0-3\n",
    "for year in range(79, 91):\n",
    "    data[f'GMom{year}'] = np.nan\n",
    "    mask = data[f'Grandmother{year}'] == 1\n",
    "    data.loc[mask, f'GMom{year}'] = 1\n",
    "    mask = (data[f'Grandmother{year}'] != 1) & (data[f'Grandmother{year}'].notna())\n",
    "    data.loc[mask, f'GMom{year}'] = 0\n",
    "\n",
    "data['GMom_0to3'] = np.nan\n",
    "for age, cols in zip(range(14, 29), [\n",
    "    ['GMom90'],\n",
    "    ['GMom89', 'GMom90'],\n",
    "    ['GMom88', 'GMom89', 'GMom90'],\n",
    "    ['GMom87', 'GMom88', 'GMom89', 'GMom90'],\n",
    "    ['GMom86', 'GMom87', 'GMom88', 'GMom89'],\n",
    "    ['GMom85', 'GMom86', 'GMom87', 'GMom88'],\n",
    "    ['GMom84', 'GMom85', 'GMom86', 'GMom87'],\n",
    "    ['GMom83', 'GMom84', 'GMom85', 'GMom86'],\n",
    "    ['GMom82', 'GMom83', 'GMom84', 'GMom85'],\n",
    "    ['GMom81', 'GMom82', 'GMom83', 'GMom84'],\n",
    "    ['GMom80', 'GMom81', 'GMom82', 'GMom83'],\n",
    "    ['GMom79', 'GMom80', 'GMom81', 'GMom82'],\n",
    "    ['GMom79', 'GMom80', 'GMom81'],\n",
    "    ['GMom79', 'GMom80'],\n",
    "    ['GMom79']\n",
    "]):\n",
    "    data.loc[data['cleaned_age_yr_104'] == age, 'GMom_0to3'] = data[cols].mean(axis=1)\n",
    "\n",
    "data.drop(columns=[f'GMom{year}' for year in range(79, 91)], inplace=True)\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "data.rename(columns={\n",
    "    'ChildCare_1stYr': 'ChildCare_1_Yr',\n",
    "    'ChildCare_Type_1stYr': 'ChildCare_Type_1_Yr',\n",
    "    'ChildCare_2ndYr': 'ChildCare_2_Yr',\n",
    "    'ChildCare_Type_2ndYr': 'ChildCare_Type_2_Yr',\n",
    "    'ChildCare_3rdYr': 'ChildCare_3_Yr',\n",
    "    'ChildCare_Type_3rdYr': 'ChildCare_Type_3_Yr'\n",
    "}, inplace=True)\n",
    "for y in range(1, 4):\n",
    "    data[f'RelCare_{y}_Yr'] = np.where((data[f'ChildCare_Type_{y}_Yr'].notna()) & (data[f'ChildCare_Type_{y}_Yr'] <= 10), 1, np.nan)\n",
    "    data[f'RelCare_{y}_Yr'] = np.where((data[f'ChildCare_{y}_Yr'].notna()) & (data[f'RelCare_{y}_Yr'] != 1), 0, data[f'RelCare_{y}_Yr'])\n",
    "    \n",
    "    data[f'NonRelCare_{y}_Yr'] = np.where((data[f'ChildCare_Type_{y}_Yr'].notna()) & (data[f'ChildCare_Type_{y}_Yr'] > 10), 1, np.nan)\n",
    "    data[f'NonRelCare_{y}_Yr'] = np.where((data[f'ChildCare_{y}_Yr'].notna()) & (data[f'NonRelCare_{y}_Yr'] != 1), 0, data[f'NonRelCare_{y}_Yr'])\n",
    "    \n",
    "    data[f'MomCare_{y}_Yr'] = np.where((data[f'RelCare_{y}_Yr'] == 0) & (data[f'NonRelCare_{y}_Yr'] == 0), 1, np.nan)\n",
    "    data[f'MomCare_{y}_Yr'] = np.where((data[f'MomCare_{y}_Yr'] != 1) & (data[f'RelCare_{y}_Yr'].notna()) & (data[f'NonRelCare_{y}_Yr'].notna()), 0, data[f'MomCare_{y}_Yr'])\n",
    "\n",
    "data['RelCare'] = data[[f'RelCare_{y}_Yr' for y in range(1, 4)]].mean(axis=1)\n",
    "data['NonRelCare'] = data[[f'NonRelCare_{y}_Yr' for y in range(1, 4)]].mean(axis=1)\n",
    "data['MomCare'] = data[[f'MomCare_{y}_Yr' for y in range(1, 4)]].mean(axis=1)\n",
    "\n",
    "\n",
    "# Mom smoked, Mom drank, Breastfed, Doctor's visit in last 3 months, Dentist ever, Weight Change during preg\n",
    "data['Alc_BefBirth'] = (data['Freq_Alc_BefBirth'] >= 3).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[data['Freq_Alc_BefBirth'].isna(), 'Alc_BefBirth'] = np.nan\n",
    "\n",
    "for x in ['Doctor', 'Dentist']:\n",
    "    data[f'{x}_temp'] = np.nan\n",
    "    mask_16_19 = (data['cleaned_age_yr_104'] <= 19) & (data['cleaned_age_yr_104'] >= 16)\n",
    "    mask_14_15 = (data['cleaned_age_yr_104'] <= 15) & (data['cleaned_age_yr_104'] >= 14)\n",
    "    mask_20_21 = (data['cleaned_age_yr_104'] >= 20) & (data['cleaned_age_yr_104'] <= 21)\n",
    "    data.loc[mask_16_19, f'{x}_temp'] = data[[f'Last_{x}86', f'Last_{x}88']].mean(axis=1)\n",
    "    data.loc[mask_14_15, f'{x}_temp'] = data[[f'Last_{x}88', f'Last_{x}90']].mean(axis=1)\n",
    "    data.loc[mask_20_21, f'{x}_temp'] = data[f'Last_{x}86']\n",
    "data['Doctor_0to3'] = np.nan\n",
    "data.loc[data['Doctor_temp'] <= 2, 'Doctor_0to3'] = 1\n",
    "data.loc[data['Doctor_temp'] > 2, 'Doctor_0to3'] = 0\n",
    "\n",
    "# Dentist visits 0 to 3 years\n",
    "data['Dentist_0to3'] = np.nan\n",
    "data.loc[data['Dentist_temp'] < 7, 'Dentist_0to3'] = 1\n",
    "data.loc[data['Dentist_temp'] == 7, 'Dentist_0to3'] = 0\n",
    "\n",
    "# Drop temporary columns\n",
    "data.drop(columns=['Doctor_temp', 'Dentist_temp'], inplace=True)\n",
    "\n",
    "# unchanged: Moth_Smoke_BefBirth, Breastfed, Moth_WeightChange\n",
    "\n",
    "\n",
    "\n",
    "# Illness in 1st year, Premature birth, Birthweight, Priv Health Insurance 0-3, Medicaid 0-3\n",
    "data['Premature'] = (data['BornEarlyorLate'] == 1).apply(lambda x: x if np.isnan(x) else int(x))\n",
    "data.loc[data['BornOnTime'].isna(), 'Premature'] = np.nan\n",
    "\n",
    "# Priv Health Insurance 0-3, Medicaid 0-3\n",
    "for x in ['Insurance', 'Medicaid']:\n",
    "    data[f'{x}_0to3'] = np.nan\n",
    "    mask_16_19 = (data['cleaned_age_yr_104'] <= 19) & (data['cleaned_age_yr_104'] >= 16)\n",
    "    mask_14_15 = (data['cleaned_age_yr_104'] <= 15) & (data['cleaned_age_yr_104'] >= 14)\n",
    "    mask_20_21 = (data['cleaned_age_yr_104'] >= 20) & (data['cleaned_age_yr_104'] <= 21)\n",
    "    data.loc[mask_16_19, f'{x}_0to3'] = data[[f'{x}86', f'{x}88']].mean(axis=1)\n",
    "    data.loc[mask_14_15, f'{x}_0to3'] = data[[f'{x}88', f'{x}90']].mean(axis=1)\n",
    "    data.loc[mask_20_21, f'{x}_0to3'] = data[f'{x}86']\n",
    "\n",
    "# Log Birth Weight\n",
    "data['logBW'] = np.log(data['BirthWeight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_age_yr_104\n",
       "21.0    183\n",
       "22.0    176\n",
       "20.0    171\n",
       "19.0    170\n",
       "23.0    156\n",
       "24.0    154\n",
       "25.0    123\n",
       "26.0     86\n",
       "27.0     75\n",
       "28.0     57\n",
       "29.0     41\n",
       "18.0     39\n",
       "30.0     14\n",
       "31.0      7\n",
       "32.0      2\n",
       "33.0      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert np.all(data[\"eligibility_siblingdifferenttreatment_90\"] == data[\"headstart_fixedeffect_indicator_90\"].notna().astype(int))\n",
    "mask = data['Sample90'] == 1\n",
    "data.loc[mask, 'cleaned_age_yr_104'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_CV\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/1785021321.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"PreTreatIndex\"] = np.nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Head Start</th>\n",
       "      <th>Other Preschool</th>\n",
       "      <th>Control Mean</th>\n",
       "      <th>Sample Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Attrit</th>\n",
       "      <td>0.02288</td>\n",
       "      <td>-0.005789</td>\n",
       "      <td>0.04087</td>\n",
       "      <td>1517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attrit se/std</th>\n",
       "      <td>0.018304</td>\n",
       "      <td>0.022234</td>\n",
       "      <td>0.198055</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attrit p-value</th>\n",
       "      <td>0.21131162682943538</td>\n",
       "      <td>0.7945851305256918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPVTat3</th>\n",
       "      <td>-1.251765</td>\n",
       "      <td>-6.89092</td>\n",
       "      <td>20.724138</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPVTat3 se/std</th>\n",
       "      <td>21.095136</td>\n",
       "      <td>18.133818</td>\n",
       "      <td>12.941011</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medicaid_0to3 se/std</th>\n",
       "      <td>0.133876</td>\n",
       "      <td>0.095794</td>\n",
       "      <td>0.447503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medicaid_0to3 p-value</th>\n",
       "      <td>0.7218903024096746</td>\n",
       "      <td>0.9460043309815034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PreTreatIndex</th>\n",
       "      <td>0.020951</td>\n",
       "      <td>0.063964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PreTreatIndex se/std</th>\n",
       "      <td>0.079848</td>\n",
       "      <td>0.073031</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PreTreatIndex p-value</th>\n",
       "      <td>0.7930263520360044</td>\n",
       "      <td>0.38111608695963406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Head Start      Other Preschool Control Mean  \\\n",
       "Attrit                             0.02288            -0.005789      0.04087   \n",
       "Attrit se/std                     0.018304             0.022234     0.198055   \n",
       "Attrit p-value         0.21131162682943538   0.7945851305256918          NaN   \n",
       "PPVTat3                          -1.251765             -6.89092    20.724138   \n",
       "PPVTat3 se/std                   21.095136            18.133818    12.941011   \n",
       "...                                    ...                  ...          ...   \n",
       "Medicaid_0to3 se/std              0.133876             0.095794     0.447503   \n",
       "Medicaid_0to3 p-value   0.7218903024096746   0.9460043309815034          NaN   \n",
       "PreTreatIndex                     0.020951             0.063964          0.0   \n",
       "PreTreatIndex se/std              0.079848             0.073031          1.0   \n",
       "PreTreatIndex p-value   0.7930263520360044  0.38111608695963406          NaN   \n",
       "\n",
       "                      Sample Size  \n",
       "Attrit                       1517  \n",
       "Attrit se/std                 NaN  \n",
       "Attrit p-value                NaN  \n",
       "PPVTat3                       261  \n",
       "PPVTat3 se/std                NaN  \n",
       "...                           ...  \n",
       "Medicaid_0to3 se/std          NaN  \n",
       "Medicaid_0to3 p-value         NaN  \n",
       "PreTreatIndex                1455  \n",
       "PreTreatIndex se/std          NaN  \n",
       "PreTreatIndex p-value         NaN  \n",
       "\n",
       "[90 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table 2\n",
    "covariates_to_list = [\"reside_with_mother_0to3\", \"prexisting_health_conditions\", \"logBW\", \"log_income_0to3\", \"log_income_at_3\", \n",
    "                      \"first_born\", \"male\", \"cleaned_age_yr_104\", \"HOME_Pct_0to3\", \"Moth_HrsWorked_BefBirth\", \n",
    "                      \"Moth_HrsWorked_0to1\", \"Father_HH_0to3\", \"GMom_0to3\", \"MomCare\", \"RelCare\", \n",
    "                      \"NonRelCare\", \"Moth_Smoke_BefBirth\", \"Alc_BefBirth\", \"Breastfed\", \"Doctor_0to3\", \n",
    "                      \"Dentist_0to3\", \"Moth_WeightChange\", \"Illness_1stYr\", \"Premature\", \"Insurance_0to3\", \n",
    "                      \"Medicaid_0to3\"]\n",
    "mask = data['Sample90'] == 1\n",
    "for covariate in covariates_to_list:\n",
    "    data[covariate + \"_CV\"] = np.nan\n",
    "    data.loc[mask, covariate + \"_CV\"] = (data.loc[mask, covariate] - data.loc[mask, covariate].mean()) / data.loc[mask, covariate].std()\n",
    "\n",
    "# reverse the sign so positive means good outcomes (in regression)\n",
    "varlist = [\"prexisting_health_conditions\", \"male\", \"cleaned_age_yr_104\", \"GMom_0to3\", \"MomCare\", \"RelCare\", \n",
    "           \"Moth_Smoke_BefBirth\", \"Alc_BefBirth\", \"Illness_1stYr\", \"Premature\", \"Medicaid_0to3\"]\n",
    "for var in varlist:\n",
    "    data[var + \"_CV\"] = -data[var + \"_CV\"]\n",
    "\n",
    "data[\"PreTreatIndex\"] = np.nan\n",
    "data.loc[mask, \"PreTreatIndex\"] = data.loc[mask, [covariate + \"_CV\" for covariate in covariates_to_list]].mean(axis=1)\n",
    "data.loc[mask, \"PreTreatIndex\"] = (data.loc[mask, \"PreTreatIndex\"] - data.loc[mask, \"PreTreatIndex\"].mean()) / data.loc[mask, \"PreTreatIndex\"].std()\n",
    "data.drop(columns=[covariate + \"_CV\" for covariate in covariates_to_list], inplace=True)\n",
    "\n",
    "covariate_names = [\"reside_with_mother_0to3\", \"prexisting_health_conditions\", \"very_low_birth_weight\", \"logBW\", \"log_income_0to3\", \n",
    "                      \"log_income_at_3\", \"first_born\", \"male\", \"cleaned_age_yr_104\", \"HOME_Pct_0to3\", \n",
    "                      \"Moth_HrsWorked_BefBirth\", \"Moth_HrsWorked_0to1\", \"Father_HH_0to3\", \"GMom_0to3\", \"MomCare\", \n",
    "                      \"RelCare\", \"NonRelCare\", \"Moth_Smoke_BefBirth\", \"Alc_BefBirth\", \"Breastfed\", \n",
    "                      \"Doctor_0to3\", \"Dentist_0to3\", \"Moth_WeightChange\", \"Illness_1stYr\", \"Premature\", \n",
    "                      \"Insurance_0to3\", \"Medicaid_0to3\", \"PreTreatIndex\"]\n",
    "column_names = [\"Head Start\", \"Other Preschool\", \"Control Mean\", \"Sample Size\"]\n",
    "covariate_names = [\"Attrit\", \"PPVTat3\"] + covariate_names\n",
    "row_names = []\n",
    "for covariate in covariate_names:\n",
    "    row_names.append(covariate)\n",
    "    row_names.append(covariate + \" se/std\")\n",
    "    row_names.append(covariate + \" p-value\")\n",
    "table2 = pd.DataFrame(index=row_names, columns=column_names)\n",
    "\n",
    "# regression\n",
    "mask = (data['Attrit'].notna()) & (data[\"eligibility_siblingdifferenttreatment_90\"] == 1) & (data[\"SampleID\"] != 12) & ((data[\"cleaned_age_yr_104\"] >= 19) | ((data[\"DOB_Yr_Child\"]==1985) & (data[\"DOB_Mo_Child\"]<8)))\n",
    "model = smf.ols(\"Attrit ~ headstart_fixedeffect_indicator_90 + preschool_fixedeffect_indicator_90 + C(MotherID)\", data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "table2.loc[\"Attrit\", \"Head Start\"] = model.params[\"headstart_fixedeffect_indicator_90\"]\n",
    "table2.loc[\"Attrit se/std\", \"Head Start\"] = model.bse[\"headstart_fixedeffect_indicator_90\"]\n",
    "table2.loc[\"Attrit p-value\", \"Head Start\"] = model.t_test(\"headstart_fixedeffect_indicator_90 = 0\").pvalue\n",
    "table2.loc[\"Attrit\", \"Other Preschool\"] = model.params[\"preschool_fixedeffect_indicator_90\"]\n",
    "table2.loc[\"Attrit se/std\", \"Other Preschool\"] = model.bse[\"preschool_fixedeffect_indicator_90\"]\n",
    "table2.loc[\"Attrit p-value\", \"Other Preschool\"] = model.t_test(\"preschool_fixedeffect_indicator_90 = 0\").pvalue\n",
    "table2.loc[\"Attrit\", \"Sample Size\"] = mask.sum()\n",
    "table2.loc[\"Attrit\", \"Control Mean\"] = data.loc[mask, \"Attrit\"].mean()\n",
    "table2.loc[\"Attrit se/std\", \"Control Mean\"] = data.loc[mask, \"Attrit\"].std()\n",
    "\n",
    "mask = (data['PPVTat3'].notna()) & (data[\"eligibility_siblingdifferenttreatment_90\"] == 1) & (data['Sample90'] == 1)\n",
    "model = smf.ols(\"PPVTat3 ~ headstart_fixedeffect_indicator_90 + preschool_fixedeffect_indicator_90 + male + first_born + cleaned_age_mo_90 + C(MotherID)\", data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "table2.loc[\"PPVTat3\", \"Head Start\"] = model.params[\"headstart_fixedeffect_indicator_90\"]\n",
    "table2.loc[\"PPVTat3 se/std\", \"Head Start\"] = model.bse[\"headstart_fixedeffect_indicator_90\"]\n",
    "table2.loc[\"PPVTat3 p-value\", \"Head Start\"] = model.t_test(\"headstart_fixedeffect_indicator_90 = 0\").pvalue\n",
    "table2.loc[\"PPVTat3\", \"Other Preschool\"] = model.params[\"preschool_fixedeffect_indicator_90\"]\n",
    "table2.loc[\"PPVTat3 se/std\", \"Other Preschool\"] = model.bse[\"preschool_fixedeffect_indicator_90\"]\n",
    "table2.loc[\"PPVTat3 p-value\", \"Other Preschool\"] = model.t_test(\"preschool_fixedeffect_indicator_90 = 0\").pvalue\n",
    "table2.loc[\"PPVTat3\", \"Sample Size\"] = mask.sum()\n",
    "table2.loc[\"PPVTat3\", \"Control Mean\"] = data.loc[mask, \"PPVTat3\"].mean()\n",
    "table2.loc[\"PPVTat3 se/std\", \"Control Mean\"] = data.loc[mask, \"PPVTat3\"].std()\n",
    "\n",
    "for covariate in covariate_names:\n",
    "    if covariate in [\"Attrit\", \"PPVTat3\"]:\n",
    "        continue\n",
    "    mask = (data[covariate].notna()) & (data[\"eligibility_siblingdifferenttreatment_90\"] == 1) & (data['Sample90'] == 1)\n",
    "    model = smf.ols(f\"{covariate} ~ headstart_fixedeffect_indicator_90 + preschool_fixedeffect_indicator_90 + C(MotherID)\", data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "    table2.loc[covariate, \"Head Start\"] = model.params[\"headstart_fixedeffect_indicator_90\"]\n",
    "    table2.loc[covariate + \" se/std\", \"Head Start\"] = model.bse[\"headstart_fixedeffect_indicator_90\"]\n",
    "    table2.loc[covariate + \" p-value\", \"Head Start\"] = model.t_test(\"headstart_fixedeffect_indicator_90 = 0\").pvalue\n",
    "    table2.loc[covariate, \"Other Preschool\"] = model.params[\"preschool_fixedeffect_indicator_90\"]\n",
    "    table2.loc[covariate + \" se/std\", \"Other Preschool\"] = model.bse[\"preschool_fixedeffect_indicator_90\"]\n",
    "    table2.loc[covariate + \" p-value\", \"Other Preschool\"] = model.t_test(\"preschool_fixedeffect_indicator_90 = 0\").pvalue\n",
    "    table2.loc[covariate, \"Sample Size\"] = mask.sum()\n",
    "    table2.loc[covariate, \"Control Mean\"] = data.loc[mask, covariate].mean()\n",
    "    table2.loc[covariate + \" se/std\", \"Control Mean\"] = data.loc[mask, covariate].std()\n",
    "\n",
    "table2.to_csv(\"table2.csv\")\n",
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates_to_impute = ['reside_with_mother_0to3', 'prexisting_health_conditions', 'very_low_birth_weight', 'logBW', 'log_income_0to3', \n",
    "                        'log_income_at_3', 'first_born', 'PPVTat3', 'HOME_Pct_0to3', 'Moth_HrsWorked_BefBirth', \n",
    "                        'Moth_HrsWorked_Avg_0to3', 'Moth_HrsWorked_0to1', 'Father_HH_0to3', 'GMom_0to3', 'MomCare', \n",
    "                        'RelCare', 'NonRelCare', 'Moth_Smoke_BefBirth', 'Alc_BefBirth', 'Breastfed', \n",
    "                        'Doctor_0to3', 'Dentist_0to3', 'Moth_WeightChange', 'Illness_1stYr', 'Premature', \n",
    "                        'Insurance_0to3', 'Medicaid_0to3']\n",
    "main_covariates = [\"male\"] + covariates_to_impute\n",
    "\n",
    "\n",
    "table_appendix = pd.DataFrame(index=[\"Test_std\", \"Noncog\", \"Sum_Adult\"] + main_covariates, columns=[\"Mean\", \"Median\", \"Std Dev\", \"Head Start Mean\", \"Other Preschool Mean\", \"No Preschool Mean\", \"Total Sample Size\"])\n",
    "for covariate in main_covariates:\n",
    "    mask = (data[covariate].notna()) & (data[\"eligibility_siblingdifferenttreatment_90\"] == 1) & (data['Sample90'] == 1)\n",
    "    table_appendix.loc[covariate, \"Total Sample Size\"] = mask.sum()\n",
    "    table_appendix.loc[covariate, \"Mean\"] = data.loc[mask, covariate].mean()\n",
    "    table_appendix.loc[covariate, \"Median\"] = data.loc[mask, covariate].median()\n",
    "    table_appendix.loc[covariate, \"Std Dev\"] = data.loc[mask, covariate].std()\n",
    "    mask = (data[covariate].notna()) & (data[\"eligibility_siblingdifferenttreatment_90\"] == 1) & (data['Sample90'] == 1) & (data[\"headstart_fixedeffect_indicator_90\"] == 1)\n",
    "    table_appendix.loc[covariate, \"Head Start Mean\"] = data.loc[mask, covariate].mean()\n",
    "    mask = (data[covariate].notna()) & (data[\"eligibility_siblingdifferenttreatment_90\"] == 1) & (data['Sample90'] == 1) & (data[\"preschool_fixedeffect_indicator_90\"] == 1)\n",
    "    table_appendix.loc[covariate, \"Other Preschool Mean\"] = data.loc[mask, covariate].mean()\n",
    "    mask = (data[covariate].notna()) & (data[\"eligibility_siblingdifferenttreatment_90\"] == 1) & (data['Sample90'] == 1) & (data[\"preschool_fixedeffect_indicator_90\"] == 0) & (data[\"headstart_fixedeffect_indicator_90\"] == 0)\n",
    "    table_appendix.loc[covariate, \"No Preschool Mean\"] = data.loc[mask, covariate].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/600107391.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[covariate + \"_missing\"] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1455\n"
     ]
    }
   ],
   "source": [
    "# impute missing values with mean\n",
    "covariates_to_impute = ['reside_with_mother_0to3', 'prexisting_health_conditions', 'very_low_birth_weight', 'logBW', 'log_income_0to3', \n",
    "                        'log_income_at_3', 'first_born', 'PPVTat3', 'HOME_Pct_0to3', 'Moth_HrsWorked_BefBirth', \n",
    "                        'Moth_HrsWorked_Avg_0to3', 'Moth_HrsWorked_0to1', 'Father_HH_0to3', 'GMom_0to3', 'MomCare', \n",
    "                        'RelCare', 'NonRelCare', 'Moth_Smoke_BefBirth', 'Alc_BefBirth', 'Breastfed', \n",
    "                        'Doctor_0to3', 'Dentist_0to3', 'Moth_WeightChange', 'Illness_1stYr', 'Premature', \n",
    "                        'Insurance_0to3', 'Medicaid_0to3']\n",
    "mask = data['Sample90'] == 1\n",
    "for covariate in covariates_to_impute:\n",
    "    data[covariate + \"_missing\"] = np.nan\n",
    "    data.loc[mask, covariate + \"_missing\"] = data.loc[mask, covariate].isna().astype(int)\n",
    "    # TODO: check the above line\n",
    "    data_temp = data.loc[mask].copy()\n",
    "    data_temp[covariate] = data_temp[covariate].replace(-np.inf, np.finfo(np.float64).min)\n",
    "    conditional_means = data_temp.groupby(['Black', 'Hispanic', 'male'])[covariate].transform('mean')\n",
    "    data.loc[mask, covariate] = data.loc[mask, covariate].fillna(conditional_means)\n",
    "\n",
    "# only consider the sample\n",
    "data = data.loc[mask]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(len(data)) # 1455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3790053760.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Longitudinal data\n",
    "for year in range(86, 106, 2):\n",
    "    data[f'AgeTest_Yr{year}'] = data[f'PPVTAge{year}'] // 12\n",
    "    data[f'Test_Pct{year}'] = data[[f'PPVT_Pct{year}', f'PIATMT_Pct{year}', f'PIATRR_Pct{year}']].mean(axis=1)\n",
    "\n",
    "# Preserve the data\n",
    "data = pd.wide_to_long(data, stubnames=['AgeTest_Yr', 'Test_Pct', 'PPVT_Raw', 'PIATMT_Raw', 'PIATRR_Raw', 'PPVT_Pct', 'PIATMT_Pct', 'PIATRR_Pct', 'BPI_Pct', 'BPIAS_Pct'], i=['ChildID'], j='Year', sep='')\n",
    "mask = (data[\"AgeTest_Yr\"] >= 5) & (data[\"AgeTest_Yr\"] <= 14)\n",
    "data = data.loc[mask]\n",
    "data.reset_index(drop=False, inplace=True)\n",
    "data[\"Group_5to6\"] = ((5 <= data[\"AgeTest_Yr\"]) & (data[\"AgeTest_Yr\"] <= 6)).astype(int)\n",
    "data[\"Group_7to10\"] = ((7 <= data[\"AgeTest_Yr\"]) & (data[\"AgeTest_Yr\"] <= 10)).astype(int)\n",
    "data[\"Group_11to14\"] = ((11 <= data[\"AgeTest_Yr\"]) & (data[\"AgeTest_Yr\"] <= 14)).astype(int)\n",
    "\n",
    "for grade in [\"5to6\", \"7to10\", \"11to14\"]:\n",
    "    mask = data[f\"Group_{grade}\"] == 1\n",
    "    for test in [\"PIATMT\", \"PIATRR\", \"PPVT\", \"BPI\", \"BPIAS\"]:\n",
    "        data[f'{test}_std_{grade}'] = np.nan\n",
    "        data.loc[mask, f'{test}_std_{grade}'] = (data.loc[mask, f'{test}_Pct'] - data.loc[mask, f'{test}_Pct'].mean()) / data.loc[mask, f'{test}_Pct'].std()\n",
    "    data[f'Test_std_{grade}'] = np.nan\n",
    "    data.loc[mask, f'Test_std_{grade}'] = data.loc[mask, [f'{t}_std_{grade}' for t in [\"PIATMT\", \"PIATRR\", \"PPVT\"]]].mean(axis=1)\n",
    "    data.loc[mask, f'Test_std_{grade}'] = (data.loc[mask, f'Test_std_{grade}'] - data.loc[mask, f'Test_std_{grade}'].mean()) / data.loc[mask, f'Test_std_{grade}'].std()\n",
    "\n",
    "for test in [\"PIATMT\", \"PIATRR\", \"PPVT\", \"BPI\", \"BPIAS\", \"Test\"]:\n",
    "    data[f'{test}_std'] = np.nan\n",
    "    data.loc[data[\"Group_5to6\"] == 1, f'{test}_std'] = data.loc[data[\"Group_5to6\"] == 1, f'{test}_std_5to6']\n",
    "    data.loc[data[\"Group_7to10\"] == 1, f'{test}_std'] = data.loc[data[\"Group_7to10\"] == 1, f'{test}_std_7to10']\n",
    "    data.loc[data[\"Group_11to14\"] == 1, f'{test}_std'] = data.loc[data[\"Group_11to14\"] == 1, f'{test}_std_11to14']\n",
    "\n",
    "data[\"non_male\"] = 1 - data[\"male\"]\n",
    "data.rename(columns={\"male\": \"Male\", \"non_male\": \"NonMale\"}, inplace=True)\n",
    "\n",
    "data[\"lowAFQT\"] = (data[\"impAFQT_std\"] <= -1).astype(int)\n",
    "data.loc[data[\"impAFQT_std\"].isna(), \"lowAFQT\"] = np.nan\n",
    "data['NonlowAFQT'] = 1-data['lowAFQT']\n",
    "\n",
    "for grade in [\"5to6\", \"7to10\", \"11to14\"]:\n",
    "    data[f'HS_{grade}'] = 0\n",
    "    mask = (data[f'Group_{grade}'] == 1) & (data['headstart_fixedeffect_indicator_90'] == 1)\n",
    "    data.loc[mask, f'HS_{grade}'] = 1\n",
    "    mask = data['headstart_fixedeffect_indicator_90'].isna()\n",
    "    data.loc[mask, f'HS_{grade}'] = np.nan\n",
    "\n",
    "    data[f\"Pre_{grade}\"] = 0\n",
    "    mask = (data[f'Group_{grade}'] == 1) & (data['preschool_fixedeffect_indicator_90'] == 1)\n",
    "    data.loc[mask, f\"Pre_{grade}\"] = 1\n",
    "    mask = data['preschool_fixedeffect_indicator_90'].isna()\n",
    "    data.loc[mask, f\"Pre_{grade}\"] = np.nan\n",
    "\n",
    "for group in [\"Male\", \"NonMale\", \"Black\", \"NonBlack\", \"lowAFQT\", \"NonlowAFQT\"]:\n",
    "    data[f'HS_{group}'] = np.nan\n",
    "    mask = (data['headstart_fixedeffect_indicator_90'] == 1) & (data[group] == 1)\n",
    "    data.loc[mask, f'HS_{group}'] = 1\n",
    "    mask = (data[f'HS_{group}'] != 1) & (data['headstart_fixedeffect_indicator_90'].notna())\n",
    "    data.loc[mask, f'HS_{group}'] = 0\n",
    "\n",
    "    data[f'Pre_{group}'] = np.nan\n",
    "    mask = (data['preschool_fixedeffect_indicator_90'] == 1) & (data[group] == 1)\n",
    "    data.loc[mask, f'Pre_{group}'] = 1\n",
    "    mask = (data[f'Pre_{group}'] != 1) & (data['preschool_fixedeffect_indicator_90'].notna())\n",
    "    data.loc[mask, f'Pre_{group}'] = 0\n",
    "\n",
    "for grade in [\"5to6\", \"7to10\", \"11to14\"]:\n",
    "    for group in [\"Male\", \"NonMale\", \"Black\", \"NonBlack\", \"lowAFQT\", \"NonlowAFQT\"]:\n",
    "        data[f'HS_{group}_{grade}'] = np.nan\n",
    "        mask = (data[f'Group_{grade}'] == 1) & (data['headstart_fixedeffect_indicator_90'] == 1) & (data[group] == 1)\n",
    "        data.loc[mask, f'HS_{group}_{grade}'] = 1\n",
    "        mask = (data[f'HS_{group}_{grade}'] != 1) & (data['headstart_fixedeffect_indicator_90'].notna())\n",
    "        data.loc[mask, f'HS_{group}_{grade}'] = 0\n",
    "\n",
    "        data[f'Pre_{group}_{grade}'] = np.nan\n",
    "        mask = (data[f'Group_{grade}'] == 1) & (data['preschool_fixedeffect_indicator_90'] == 1) & (data[group] == 1)\n",
    "        data.loc[mask, f'Pre_{group}_{grade}'] = 1\n",
    "        mask = (data[f'Pre_{group}_{grade}'] != 1) & (data['preschool_fixedeffect_indicator_90'].notna())\n",
    "        data.loc[mask, f'Pre_{group}_{grade}'] = 0\n",
    "\n",
    "data[\"Group_5to14\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Head Start 5to6</th>\n",
       "      <td>-0.062599</td>\n",
       "      <td>0.060392</td>\n",
       "      <td>0.067084</td>\n",
       "      <td>0.130202</td>\n",
       "      <td>0.146343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start 5to6 se/std</th>\n",
       "      <td>0.085362</td>\n",
       "      <td>0.078521</td>\n",
       "      <td>0.075246</td>\n",
       "      <td>0.090472</td>\n",
       "      <td>0.089245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start 5to6 p-value</th>\n",
       "      <td>0.46335288380337414</td>\n",
       "      <td>0.4418248532361223</td>\n",
       "      <td>0.37264916470467</td>\n",
       "      <td>0.15011289240066253</td>\n",
       "      <td>0.10104822728752048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start 7to10</th>\n",
       "      <td>-0.148909</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.034212</td>\n",
       "      <td>0.113062</td>\n",
       "      <td>0.127851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start 7to10 se/std</th>\n",
       "      <td>0.068054</td>\n",
       "      <td>0.062423</td>\n",
       "      <td>0.059222</td>\n",
       "      <td>0.063253</td>\n",
       "      <td>0.063647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start 7to10 p-value</th>\n",
       "      <td>0.028662982358712192</td>\n",
       "      <td>0.9448021824111316</td>\n",
       "      <td>0.5634659866932015</td>\n",
       "      <td>0.07386212095851483</td>\n",
       "      <td>0.04456165774594422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start 11to14</th>\n",
       "      <td>-0.23041</td>\n",
       "      <td>-0.080992</td>\n",
       "      <td>-0.048434</td>\n",
       "      <td>0.032131</td>\n",
       "      <td>0.050266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start 11to14 se/std</th>\n",
       "      <td>0.066189</td>\n",
       "      <td>0.060988</td>\n",
       "      <td>0.058485</td>\n",
       "      <td>0.064496</td>\n",
       "      <td>0.065064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start 11to14 p-value</th>\n",
       "      <td>0.0004993915975453988</td>\n",
       "      <td>0.18417763863376824</td>\n",
       "      <td>0.40758727897899394</td>\n",
       "      <td>0.6183491832691793</td>\n",
       "      <td>0.43978184138694987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Preschool 5to6</th>\n",
       "      <td>0.295608</td>\n",
       "      <td>0.085168</td>\n",
       "      <td>0.040384</td>\n",
       "      <td>-0.062856</td>\n",
       "      <td>-0.034753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Preschool 5to6 se/std</th>\n",
       "      <td>0.08278</td>\n",
       "      <td>0.077064</td>\n",
       "      <td>0.074756</td>\n",
       "      <td>0.086215</td>\n",
       "      <td>0.086667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Preschool 5to6 p-value</th>\n",
       "      <td>0.0003556304769790185</td>\n",
       "      <td>0.26908968302419434</td>\n",
       "      <td>0.5890503236636988</td>\n",
       "      <td>0.4659612060479996</td>\n",
       "      <td>0.688422618589044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Preschool 7to10</th>\n",
       "      <td>0.286541</td>\n",
       "      <td>0.103419</td>\n",
       "      <td>0.075988</td>\n",
       "      <td>0.014279</td>\n",
       "      <td>0.033746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Preschool 7to10 se/std</th>\n",
       "      <td>0.068001</td>\n",
       "      <td>0.059814</td>\n",
       "      <td>0.057074</td>\n",
       "      <td>0.064114</td>\n",
       "      <td>0.066156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Preschool 7to10 p-value</th>\n",
       "      <td>2.511559345648684e-05</td>\n",
       "      <td>0.08380787116760302</td>\n",
       "      <td>0.18305326346139805</td>\n",
       "      <td>0.8237587062499727</td>\n",
       "      <td>0.6099846080937569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Preschool 11to14</th>\n",
       "      <td>0.219584</td>\n",
       "      <td>0.055235</td>\n",
       "      <td>0.022482</td>\n",
       "      <td>-0.041958</td>\n",
       "      <td>-0.025147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Preschool 11to14 se/std</th>\n",
       "      <td>0.070053</td>\n",
       "      <td>0.064214</td>\n",
       "      <td>0.061836</td>\n",
       "      <td>0.069569</td>\n",
       "      <td>0.071551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other Preschool 11to14 p-value</th>\n",
       "      <td>0.0017213296538192327</td>\n",
       "      <td>0.38969532660962125</td>\n",
       "      <td>0.7161750694607363</td>\n",
       "      <td>0.5464332593772978</td>\n",
       "      <td>0.7252478089046317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Permanent income (standardized)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.127597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Permanent income (standardized) se/std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Permanent income (standardized) p-value</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02403570050068067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maternal AFQT (standardized)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.296025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maternal AFQT (standardized) se/std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.053061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maternal AFQT (standardized) p-value</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.419741482460619e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mom high school</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.124133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mom high school se/std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mom high school p-value</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.05791968802799056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mom some college</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.275629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mom some college se/std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mom some college p-value</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00021397243970290897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p (all age effects equalâ€”Head Start)</th>\n",
       "      <td>0.066299</td>\n",
       "      <td>0.082579</td>\n",
       "      <td>0.128126</td>\n",
       "      <td>0.13691</td>\n",
       "      <td>0.162958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pre-treatment covariates</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sibling fixed effects</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>0.040954</td>\n",
       "      <td>0.213722</td>\n",
       "      <td>0.271553</td>\n",
       "      <td>0.613102</td>\n",
       "      <td>0.622584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample size</th>\n",
       "      <td>1455</td>\n",
       "      <td>1455</td>\n",
       "      <td>1455</td>\n",
       "      <td>1455</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             1  \\\n",
       "Head Start 5to6                                      -0.062599   \n",
       "Head Start 5to6 se/std                                0.085362   \n",
       "Head Start 5to6 p-value                    0.46335288380337414   \n",
       "Head Start 7to10                                     -0.148909   \n",
       "Head Start 7to10 se/std                               0.068054   \n",
       "Head Start 7to10 p-value                  0.028662982358712192   \n",
       "Head Start 11to14                                     -0.23041   \n",
       "Head Start 11to14 se/std                              0.066189   \n",
       "Head Start 11to14 p-value                0.0004993915975453988   \n",
       "Other Preschool 5to6                                  0.295608   \n",
       "Other Preschool 5to6 se/std                            0.08278   \n",
       "Other Preschool 5to6 p-value             0.0003556304769790185   \n",
       "Other Preschool 7to10                                 0.286541   \n",
       "Other Preschool 7to10 se/std                          0.068001   \n",
       "Other Preschool 7to10 p-value            2.511559345648684e-05   \n",
       "Other Preschool 11to14                                0.219584   \n",
       "Other Preschool 11to14 se/std                         0.070053   \n",
       "Other Preschool 11to14 p-value           0.0017213296538192327   \n",
       "Permanent income (standardized)                            NaN   \n",
       "Permanent income (standardized) se/std                     NaN   \n",
       "Permanent income (standardized) p-value                    NaN   \n",
       "Maternal AFQT (standardized)                               NaN   \n",
       "Maternal AFQT (standardized) se/std                        NaN   \n",
       "Maternal AFQT (standardized) p-value                       NaN   \n",
       "Mom high school                                            NaN   \n",
       "Mom high school se/std                                     NaN   \n",
       "Mom high school p-value                                    NaN   \n",
       "Mom some college                                           NaN   \n",
       "Mom some college se/std                                    NaN   \n",
       "Mom some college p-value                                   NaN   \n",
       "p (all age effects equalâ€”Head Start)                  0.066299   \n",
       "Pre-treatment covariates                                 False   \n",
       "Sibling fixed effects                                    False   \n",
       "R2                                                    0.040954   \n",
       "Sample size                                               1455   \n",
       "\n",
       "                                                           2  \\\n",
       "Head Start 5to6                                     0.060392   \n",
       "Head Start 5to6 se/std                              0.078521   \n",
       "Head Start 5to6 p-value                   0.4418248532361223   \n",
       "Head Start 7to10                                    0.004322   \n",
       "Head Start 7to10 se/std                             0.062423   \n",
       "Head Start 7to10 p-value                  0.9448021824111316   \n",
       "Head Start 11to14                                  -0.080992   \n",
       "Head Start 11to14 se/std                            0.060988   \n",
       "Head Start 11to14 p-value                0.18417763863376824   \n",
       "Other Preschool 5to6                                0.085168   \n",
       "Other Preschool 5to6 se/std                         0.077064   \n",
       "Other Preschool 5to6 p-value             0.26908968302419434   \n",
       "Other Preschool 7to10                               0.103419   \n",
       "Other Preschool 7to10 se/std                        0.059814   \n",
       "Other Preschool 7to10 p-value            0.08380787116760302   \n",
       "Other Preschool 11to14                              0.055235   \n",
       "Other Preschool 11to14 se/std                       0.064214   \n",
       "Other Preschool 11to14 p-value           0.38969532660962125   \n",
       "Permanent income (standardized)                          NaN   \n",
       "Permanent income (standardized) se/std                   NaN   \n",
       "Permanent income (standardized) p-value                  NaN   \n",
       "Maternal AFQT (standardized)                             NaN   \n",
       "Maternal AFQT (standardized) se/std                      NaN   \n",
       "Maternal AFQT (standardized) p-value                     NaN   \n",
       "Mom high school                                          NaN   \n",
       "Mom high school se/std                                   NaN   \n",
       "Mom high school p-value                                  NaN   \n",
       "Mom some college                                         NaN   \n",
       "Mom some college se/std                                  NaN   \n",
       "Mom some college p-value                                 NaN   \n",
       "p (all age effects equalâ€”Head Start)                0.082579   \n",
       "Pre-treatment covariates                                True   \n",
       "Sibling fixed effects                                  False   \n",
       "R2                                                  0.213722   \n",
       "Sample size                                             1455   \n",
       "\n",
       "                                                              3  \\\n",
       "Head Start 5to6                                        0.067084   \n",
       "Head Start 5to6 se/std                                 0.075246   \n",
       "Head Start 5to6 p-value                        0.37264916470467   \n",
       "Head Start 7to10                                       0.034212   \n",
       "Head Start 7to10 se/std                                0.059222   \n",
       "Head Start 7to10 p-value                     0.5634659866932015   \n",
       "Head Start 11to14                                     -0.048434   \n",
       "Head Start 11to14 se/std                               0.058485   \n",
       "Head Start 11to14 p-value                   0.40758727897899394   \n",
       "Other Preschool 5to6                                   0.040384   \n",
       "Other Preschool 5to6 se/std                            0.074756   \n",
       "Other Preschool 5to6 p-value                 0.5890503236636988   \n",
       "Other Preschool 7to10                                  0.075988   \n",
       "Other Preschool 7to10 se/std                           0.057074   \n",
       "Other Preschool 7to10 p-value               0.18305326346139805   \n",
       "Other Preschool 11to14                                 0.022482   \n",
       "Other Preschool 11to14 se/std                          0.061836   \n",
       "Other Preschool 11to14 p-value               0.7161750694607363   \n",
       "Permanent income (standardized)                        0.127597   \n",
       "Permanent income (standardized) se/std                 0.056545   \n",
       "Permanent income (standardized) p-value     0.02403570050068067   \n",
       "Maternal AFQT (standardized)                           0.296025   \n",
       "Maternal AFQT (standardized) se/std                    0.053061   \n",
       "Maternal AFQT (standardized) p-value      2.419741482460619e-08   \n",
       "Mom high school                                        0.124133   \n",
       "Mom high school se/std                                  0.06546   \n",
       "Mom high school p-value                     0.05791968802799056   \n",
       "Mom some college                                       0.275629   \n",
       "Mom some college se/std                                0.074456   \n",
       "Mom some college p-value                 0.00021397243970290897   \n",
       "p (all age effects equalâ€”Head Start)                   0.128126   \n",
       "Pre-treatment covariates                                   True   \n",
       "Sibling fixed effects                                     False   \n",
       "R2                                                     0.271553   \n",
       "Sample size                                                1455   \n",
       "\n",
       "                                                           4  \\\n",
       "Head Start 5to6                                     0.130202   \n",
       "Head Start 5to6 se/std                              0.090472   \n",
       "Head Start 5to6 p-value                  0.15011289240066253   \n",
       "Head Start 7to10                                    0.113062   \n",
       "Head Start 7to10 se/std                             0.063253   \n",
       "Head Start 7to10 p-value                 0.07386212095851483   \n",
       "Head Start 11to14                                   0.032131   \n",
       "Head Start 11to14 se/std                            0.064496   \n",
       "Head Start 11to14 p-value                 0.6183491832691793   \n",
       "Other Preschool 5to6                               -0.062856   \n",
       "Other Preschool 5to6 se/std                         0.086215   \n",
       "Other Preschool 5to6 p-value              0.4659612060479996   \n",
       "Other Preschool 7to10                               0.014279   \n",
       "Other Preschool 7to10 se/std                        0.064114   \n",
       "Other Preschool 7to10 p-value             0.8237587062499727   \n",
       "Other Preschool 11to14                             -0.041958   \n",
       "Other Preschool 11to14 se/std                       0.069569   \n",
       "Other Preschool 11to14 p-value            0.5464332593772978   \n",
       "Permanent income (standardized)                          NaN   \n",
       "Permanent income (standardized) se/std                   NaN   \n",
       "Permanent income (standardized) p-value                  NaN   \n",
       "Maternal AFQT (standardized)                             NaN   \n",
       "Maternal AFQT (standardized) se/std                      NaN   \n",
       "Maternal AFQT (standardized) p-value                     NaN   \n",
       "Mom high school                                          NaN   \n",
       "Mom high school se/std                                   NaN   \n",
       "Mom high school p-value                                  NaN   \n",
       "Mom some college                                         NaN   \n",
       "Mom some college se/std                                  NaN   \n",
       "Mom some college p-value                                 NaN   \n",
       "p (all age effects equalâ€”Head Start)                 0.13691   \n",
       "Pre-treatment covariates                               False   \n",
       "Sibling fixed effects                                   True   \n",
       "R2                                                  0.613102   \n",
       "Sample size                                             1455   \n",
       "\n",
       "                                                           5  \n",
       "Head Start 5to6                                     0.146343  \n",
       "Head Start 5to6 se/std                              0.089245  \n",
       "Head Start 5to6 p-value                  0.10104822728752048  \n",
       "Head Start 7to10                                    0.127851  \n",
       "Head Start 7to10 se/std                             0.063647  \n",
       "Head Start 7to10 p-value                 0.04456165774594422  \n",
       "Head Start 11to14                                   0.050266  \n",
       "Head Start 11to14 se/std                            0.065064  \n",
       "Head Start 11to14 p-value                0.43978184138694987  \n",
       "Other Preschool 5to6                               -0.034753  \n",
       "Other Preschool 5to6 se/std                         0.086667  \n",
       "Other Preschool 5to6 p-value               0.688422618589044  \n",
       "Other Preschool 7to10                               0.033746  \n",
       "Other Preschool 7to10 se/std                        0.066156  \n",
       "Other Preschool 7to10 p-value             0.6099846080937569  \n",
       "Other Preschool 11to14                             -0.025147  \n",
       "Other Preschool 11to14 se/std                       0.071551  \n",
       "Other Preschool 11to14 p-value            0.7252478089046317  \n",
       "Permanent income (standardized)                          NaN  \n",
       "Permanent income (standardized) se/std                   NaN  \n",
       "Permanent income (standardized) p-value                  NaN  \n",
       "Maternal AFQT (standardized)                             NaN  \n",
       "Maternal AFQT (standardized) se/std                      NaN  \n",
       "Maternal AFQT (standardized) p-value                     NaN  \n",
       "Mom high school                                          NaN  \n",
       "Mom high school se/std                                   NaN  \n",
       "Mom high school p-value                                  NaN  \n",
       "Mom some college                                         NaN  \n",
       "Mom some college se/std                                  NaN  \n",
       "Mom some college p-value                                 NaN  \n",
       "p (all age effects equalâ€”Head Start)                0.162958  \n",
       "Pre-treatment covariates                                True  \n",
       "Sibling fixed effects                                   True  \n",
       "R2                                                  0.622584  \n",
       "Sample size                                             1455  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table 3\n",
    "column_names = [str(i) for i in range(1, 6)]\n",
    "_row_names = [\"Head Start 5to6\", \"Head Start 7to10\", \"Head Start 11to14\", \"Other Preschool 5to6\", \"Other Preschool 7to10\", \"Other Preschool 11to14\"]\n",
    "_row_names += [\"Permanent income (standardized)\", \"Maternal AFQT (standardized)\", \"Mom high school\", \"Mom some college\"]\n",
    "row_names = []\n",
    "for row_name in _row_names:\n",
    "    row_names.append(row_name)\n",
    "    row_names.append(row_name + \" se/std\")\n",
    "    row_names.append(row_name + \" p-value\")\n",
    "row_names += [\"p (all age effects equalâ€”Head Start)\", \"Pre-treatment covariates\", \"Sibling fixed effects\", \"R2\", \"Sample size\"]\n",
    "\n",
    "table3 = pd.DataFrame(index=row_names, columns=column_names)\n",
    "\n",
    "\n",
    "def _generate_column_default(table3, model, idx):\n",
    "    table3.loc[\"Head Start 5to6\", str(idx)] = model.params[\"HS_5to6\"]\n",
    "    table3.loc[\"Head Start 5to6 se/std\", str(idx)] = model.bse[\"HS_5to6\"]\n",
    "    table3.loc[\"Head Start 5to6 p-value\", str(idx)] = model.t_test(\"HS_5to6 = 0\").pvalue\n",
    "    table3.loc[\"Head Start 7to10\", str(idx)] = model.params[\"HS_7to10\"]\n",
    "    table3.loc[\"Head Start 7to10 se/std\", str(idx)] = model.bse[\"HS_7to10\"]\n",
    "    table3.loc[\"Head Start 7to10 p-value\", str(idx)] = model.t_test(\"HS_7to10 = 0\").pvalue\n",
    "    table3.loc[\"Head Start 11to14\", str(idx)] = model.params[\"HS_11to14\"]\n",
    "    table3.loc[\"Head Start 11to14 se/std\", str(idx)] = model.bse[\"HS_11to14\"]\n",
    "    table3.loc[\"Head Start 11to14 p-value\", str(idx)] = model.t_test(\"HS_11to14 = 0\").pvalue\n",
    "    table3.loc[\"Other Preschool 5to6\", str(idx)] = model.params[\"Pre_5to6\"]\n",
    "    table3.loc[\"Other Preschool 5to6 se/std\", str(idx)] = model.bse[\"Pre_5to6\"]\n",
    "    table3.loc[\"Other Preschool 5to6 p-value\", str(idx)] = model.t_test(\"Pre_5to6 = 0\").pvalue\n",
    "    table3.loc[\"Other Preschool 7to10\", str(idx)] = model.params[\"Pre_7to10\"]\n",
    "    table3.loc[\"Other Preschool 7to10 se/std\", str(idx)] = model.bse[\"Pre_7to10\"]\n",
    "    table3.loc[\"Other Preschool 7to10 p-value\", str(idx)] = model.t_test(\"Pre_7to10 = 0\").pvalue\n",
    "    table3.loc[\"Other Preschool 11to14\", str(idx)] = model.params[\"Pre_11to14\"]\n",
    "    table3.loc[\"Other Preschool 11to14 se/std\", str(idx)] = model.bse[\"Pre_11to14\"]\n",
    "    table3.loc[\"Other Preschool 11to14 p-value\", str(idx)] = model.t_test(\"Pre_11to14 = 0\").pvalue\n",
    "    table3.loc[\"p (all age effects equalâ€”Head Start)\", str(idx)] = model.f_test(\"HS_5to6 = HS_7to10 = HS_11to14\").pvalue\n",
    "    table3.loc[\"R2\", str(idx)] = model.rsquared\n",
    "    table3.loc[\"Sample size\", str(idx)] = 1455 # TODO: hardcoded value\n",
    "\n",
    "\n",
    "mask = data['Test_std'].notna()\n",
    "baseline_covariates = [\"HS_5to6\", \"HS_7to10\", \"HS_11to14\", \"Pre_5to6\", \"Pre_7to10\", \"Pre_11to14\", \"Male\", \"C(AgeTest_Yr)\", \"C(Year)\"]\n",
    "standard_covariates = covariates_to_impute + [cov + \"_missing\" for cov in covariates_to_impute]\n",
    "additional_covariates = [\"permanent_family_income_std\", \"impAFQT_std\", \"mother_highschool\", \"mother_somecollege\"]\n",
    "family_fixed_effects = [\"C(MotherID)\"]\n",
    "# NOTE: in the paper, the author claims to adjust for first_born status, but this is not the case in the code\n",
    "\n",
    "# column 1\n",
    "model = smf.ols(\"Test_std ~ \" + \" + \".join(baseline_covariates), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "_generate_column_default(table3, model, 1)\n",
    "table3.loc[\"Pre-treatment covariates\", \"1\"] = False\n",
    "table3.loc[\"Sibling fixed effects\", \"1\"] = False\n",
    "\n",
    "# column 2\n",
    "model = smf.ols(\"Test_std ~ \" + \" + \".join(baseline_covariates + standard_covariates), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "_generate_column_default(table3, model, 2)\n",
    "table3.loc[\"Pre-treatment covariates\", \"2\"] = True\n",
    "table3.loc[\"Sibling fixed effects\", \"2\"] = False\n",
    "\n",
    "# column 3\n",
    "model = smf.ols(\"Test_std ~ \" + \" + \".join(baseline_covariates + standard_covariates + additional_covariates), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "_generate_column_default(table3, model, 3)\n",
    "table3.loc[\"Pre-treatment covariates\", \"3\"] = True\n",
    "table3.loc[\"Sibling fixed effects\", \"3\"] = False\n",
    "table3.loc[\"Permanent income (standardized)\", \"3\"] = model.params[\"permanent_family_income_std\"]\n",
    "table3.loc[\"Permanent income (standardized) se/std\", \"3\"] = model.bse[\"permanent_family_income_std\"]\n",
    "table3.loc[\"Permanent income (standardized) p-value\", \"3\"] = model.t_test(\"permanent_family_income_std = 0\").pvalue\n",
    "table3.loc[\"Maternal AFQT (standardized)\", \"3\"] = model.params[\"impAFQT_std\"]\n",
    "table3.loc[\"Maternal AFQT (standardized) se/std\", \"3\"] = model.bse[\"impAFQT_std\"]\n",
    "table3.loc[\"Maternal AFQT (standardized) p-value\", \"3\"] = model.t_test(\"impAFQT_std = 0\").pvalue\n",
    "table3.loc[\"Mom high school\", \"3\"] = model.params[\"mother_highschool\"]\n",
    "table3.loc[\"Mom high school se/std\", \"3\"] = model.bse[\"mother_highschool\"]\n",
    "table3.loc[\"Mom high school p-value\", \"3\"] = model.t_test(\"mother_highschool = 0\").pvalue\n",
    "table3.loc[\"Mom some college\", \"3\"] = model.params[\"mother_somecollege\"]\n",
    "table3.loc[\"Mom some college se/std\", \"3\"] = model.bse[\"mother_somecollege\"]\n",
    "table3.loc[\"Mom some college p-value\", \"3\"] = model.t_test(\"mother_somecollege = 0\").pvalue\n",
    "\n",
    "# column 4\n",
    "model = smf.ols(\"Test_std ~ \" + \" + \".join(baseline_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "_generate_column_default(table3, model, 4)\n",
    "table3.loc[\"Pre-treatment covariates\", \"4\"] = False\n",
    "table3.loc[\"Sibling fixed effects\", \"4\"] = True\n",
    "\n",
    "# column 5\n",
    "model = smf.ols(\"Test_std ~ \" + \" + \".join(baseline_covariates + standard_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "_generate_column_default(table3, model, 5)\n",
    "table3.loc[\"Pre-treatment covariates\", \"5\"] = True\n",
    "table3.loc[\"Sibling fixed effects\", \"5\"] = True\n",
    "\n",
    "table3.to_csv(\"table3.csv\")\n",
    "table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Repeat98\"] = data[[\"Repeat_K98\"] + [f\"Repeat_{98 + 100*i}\" for i in range(1,9)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Repeat\"] = data[[f\"Repeat{yr}\" for yr in range(88, 106, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'tempLD{year}'] = data[f'LD{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'tempLD{year}'] = data[f'LD{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'tempLD{year}'] = data[f'LD{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'tempLD{year}'] = data[f'LD{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'tempLD{year}'] = data[f'LD{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'tempLD{year}'] = data[f'LD{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'tempLD{year}'] = data[f'LD{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'tempLD{year}'] = data[f'LD{year}']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"LD\"] = data[[f'tempLD{year}' for year in range(86, 102, 2)]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"LD_before\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['HSGrad'] = ((data['YA_Educ104'] >= 12) & (data['YA_Educ104'].notna())).astype(float)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['GED'] = data[[col for col in data.columns if col.startswith('GED')]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['HSGrad_GED'] = data['HSGrad']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['HighGradeAtt'] = data[[col for col in data.columns if col.startswith('HighGrade_Att')]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['someCollAtt'] = ((data['YA_Educ104'] >= 13) & (data['YA_Educ104'].notna())).astype(int)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Wages'] = data['Wages104']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'PosWages{x}'] = (data[f'Wages{x}'] > 0) & (data[f'Wages{x}'].notna())\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[data[f'Wages{x}'].isna(), f'PosWages{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'PosWages{x}'] = (data[f'Wages{x}'] > 0) & (data[f'Wages{x}'].notna())\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[data[f'Wages{x}'].isna(), f'PosWages{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'PosWages{x}'] = (data[f'Wages{x}'] > 0) & (data[f'Wages{x}'].notna())\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[data[f'Wages{x}'].isna(), f'PosWages{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'PosWages{x}'] = (data[f'Wages{x}'] > 0) & (data[f'Wages{x}'].notna())\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[data[f'Wages{x}'].isna(), f'PosWages{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'PosWages{x}'] = (data[f'Wages{x}'] > 0) & (data[f'Wages{x}'].notna())\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[data[f'Wages{x}'].isna(), f'PosWages{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:55: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'PosWages{x}'] = (data[f'Wages{x}'] > 0) & (data[f'Wages{x}'].notna())\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  data.loc[data[f'Wages{x}'].isna(), f'PosWages{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:57: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PosWages'] = data['PosWages104']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['InSchool'] = data['InSchool104']\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['Idle'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Convicted\"] = data[[col for col in data.columns if col.startswith('Convicted')]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Probation\"] = data[[col for col in data.columns if col.startswith('Probation')]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:87: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Sentenced\"] = data[[col for col in data.columns if col.startswith('Sentenced')]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Prison{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Prison{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Prison{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Prison{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Prison{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[f'Prison{x}'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Prison\"] = data[[f'Prison{x}' for x in [94, 96, 98, 100, 102, 104]]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Crime\"] = data[[\"Convicted\", \"Probation\", \"Sentenced\", \"Prison\"]].max(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"TeenPreg\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:102: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['HealthReport'] = data[[col for col in data.columns if col.startswith('Health_Report')]].mean(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:103: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['PoorHealth'] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:109: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:110: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:113: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Sum_Adult\"] = data[[outcome + \"_std\" for outcome in [\"HSGrad\", \"someCollAtt\", \"Idle\", \"Crime\", \"TeenPreg\", \"PoorHealth\"]]].sum(axis=1)\n",
      "/var/folders/nd/k3zbj8ys0v544bkn7b8pjnf80000gn/T/ipykernel_74938/3151906628.py:115: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[\"Noncog\"] = data[[outcome + \"_std\" for outcome in [\"LD\", \"Repeat\"]]].sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "# nonscore, 5-14\n",
    "# Repeat grade\n",
    "data.loc[data[\"Repeat92\"] == 6, \"Repeat92\"] = np.nan\n",
    "data[\"Repeat94\"] = data[[\"Repeat_K94\"] + [f\"Repeat_{94 + 100*i}\" for i in range(1,9)]].max(axis=1)\n",
    "data[\"Repeat96\"] = data[[\"Repeat_K96\"] + [f\"Repeat_{96 + 100*i}\" for i in range(1,13)]].max(axis=1)\n",
    "data.loc[data[\"Repeat_None94\"] == 1, \"Repeat94\"] = 0\n",
    "data.loc[data[\"Repeat_None96\"] == 1, \"Repeat96\"] = 0\n",
    "data.rename(columns={\"RepeatNone98\":\"Repeat_None98\"}, inplace=True)\n",
    "del data[\"Repeat_YA98\"]\n",
    "data[\"Repeat98\"] = data[[\"Repeat_K98\"] + [f\"Repeat_{98 + 100*i}\" for i in range(1,9)]].max(axis=1)\n",
    "data.loc[data[\"Repeat_None98\"] == 1, \"Repeat98\"] = 0\n",
    "\n",
    "data[\"Repeat\"] = data[[f\"Repeat{yr}\" for yr in range(88, 106, 2)]].max(axis=1)\n",
    "\n",
    "# Learning Disability\n",
    "for year in range(86, 102, 2):\n",
    "\tdata[f'tempLD{year}'] = data[f'LD{year}']\n",
    "\tmask = (data[f'HealthCond{year}'].notna()) & (data[f'tempLD{year}'] != 1)\n",
    "\tdata.loc[mask, f'tempLD{year}'] = 0\n",
    "data[\"LD\"] = data[[f'tempLD{year}' for year in range(86, 102, 2)]].max(axis=1)\n",
    "\n",
    "data[\"LD_before\"] = np.nan\n",
    "for year in range(86, 102, 2):\n",
    "    mask = (data[f'tempLD{year}'] == 1) & (data[f'cleaned_age_yr_{year}'] < 5)\n",
    "    data.loc[mask, \"LD_before\"] = 1\n",
    "\n",
    "data.loc[data[\"LD_before\"] == 1, \"LD\"] = np.nan\n",
    "data.drop(columns=[f'tempLD{year}' for year in range(86, 102, 2)], inplace=True)\n",
    "data.drop(columns=[col for col in data.columns if col.startswith('temp')], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Long term\n",
    "# HS Grad\n",
    "data['HSGrad'] = ((data['YA_Educ104'] >= 12) & (data['YA_Educ104'].notna())).astype(float)\n",
    "data.loc[data['YA_Educ104'].isna(), 'HSGrad'] = np.nan\n",
    "data['GED'] = data[[col for col in data.columns if col.startswith('GED')]].max(axis=1)\n",
    "data['HSGrad_GED'] = data['HSGrad']\n",
    "data.loc[(data['HSGrad_GED'] == 1) & (data['GED'] == 2), 'HSGrad_GED'] = np.nan\n",
    "\n",
    "# College\n",
    "data['HighGradeAtt'] = data[[col for col in data.columns if col.startswith('HighGrade_Att')]].max(axis=1)\n",
    "data['someCollAtt'] = ((data['YA_Educ104'] >= 13) & (data['YA_Educ104'].notna())).astype(int)\n",
    "data.loc[data['YA_Educ104'].isna(), 'someCollAtt'] = np.nan\n",
    "data.loc[(data['HighGradeAtt'] > 12) & (data['HighGradeAtt'].notna()), 'someCollAtt'] = 1\n",
    "\n",
    "# Idleness\n",
    "data['Wages'] = data['Wages104']\n",
    "data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 2002), 'Wages'] = data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 2002), 'Wages102']\n",
    "data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 2000), 'Wages'] = data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 2000), 'Wages100']\n",
    "data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 1998), 'Wages'] = data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 1998), 'Wages98']\n",
    "data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 1996), 'Wages'] = data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 1996), 'Wages96']\n",
    "data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 1994), 'Wages'] = data.loc[(data['Wages'].isna()) & (data['YA_LastInterview'] == 1994), 'Wages94']\n",
    "for x in [104, 102, 100, 98, 96, 94]:\n",
    "\tdata[f'PosWages{x}'] = (data[f'Wages{x}'] > 0) & (data[f'Wages{x}'].notna())\n",
    "\tdata.loc[data[f'Wages{x}'].isna(), f'PosWages{x}'] = np.nan\n",
    "data['PosWages'] = data['PosWages104']\n",
    "data.loc[data['PosWages104'].isna() & (data['Wages_Est104'] == 1), 'PosWages'] = 0\n",
    "data.loc[data['PosWages104'].isna() & data['Wages_Est104'].notna() & (data['Wages_Est104'] > 1), 'PosWages'] = 1\n",
    "data.loc[data['PosWages'].isna() & (data['YA_LastInterview'] == 2002), 'PosWages'] = data.loc[data['PosWages'].isna() & (data['YA_LastInterview'] == 2002), 'PosWages102']\n",
    "data.loc[data['PosWages'].isna() & data['PosWages102'].isna() & (data['Wages_Est102'] == 1) & (data['YA_LastInterview'] == 2002), 'PosWages'] = 0\n",
    "data.loc[data['PosWages'].isna() & data['PosWages102'].isna() & data['Wages_Est102'].notna() & (data['Wages_Est102'] > 1) & (data['YA_LastInterview'] == 2002), 'PosWages'] = 1\n",
    "data.loc[data['PosWages'].isna() & (data['YA_LastInterview'] == 2000), 'PosWages'] = data.loc[data['PosWages'].isna() & (data['YA_LastInterview'] == 2000), 'PosWages100']\n",
    "data.loc[data['PosWages'].isna() & data['PosWages100'].isna() & (data['Wages_Est100'] == 1) & (data['YA_LastInterview'] == 2000), 'PosWages'] = 0\n",
    "data.loc[data['PosWages'].isna() & data['PosWages100'].isna() & data['Wages_Est100'].notna() & (data['Wages_Est100'] > 1) & (data['YA_LastInterview'] == 2000), 'PosWages'] = 1\n",
    "\n",
    "data.loc[(data['PosWages'].isna()) & (data['YA_LastInterview'] == 1998), 'PosWages'] = data.loc[(data['PosWages'].isna()) & (data['YA_LastInterview'] == 1998), 'PosWages98']\n",
    "data.loc[(data['PosWages'].isna()) & (data['YA_LastInterview'] == 1996), 'PosWages'] = data.loc[(data['PosWages'].isna()) & (data['YA_LastInterview'] == 1996), 'PosWages96']\n",
    "data.loc[(data['PosWages'].isna()) & (data['YA_LastInterview'] == 1994), 'PosWages'] = data.loc[(data['PosWages'].isna()) & (data['YA_LastInterview'] == 1994), 'PosWages94']\n",
    "\n",
    "data['InSchool'] = data['InSchool104']\n",
    "data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 2002), 'InSchool'] = data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 2002), 'InSchool102']\n",
    "data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 2000), 'InSchool'] = data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 2000), 'InSchool100']\n",
    "data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 1998), 'InSchool'] = data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 1998), 'InSchool98']\n",
    "data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 1996), 'InSchool'] = data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 1996), 'InSchool96']\n",
    "data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 1994), 'InSchool'] = data.loc[(data['InSchool'].isna()) & (data['YA_LastInterview'] == 1994), 'InSchool94']\n",
    "\n",
    "data['Idle'] = np.nan\n",
    "data.loc[data[\"InSchool\"].notna() | data[\"PosWages\"].notna(), \"Idle\"] = 0\n",
    "data.loc[(data[\"InSchool\"] == 0) & (data[\"PosWages\"] == 0), \"Idle\"] = 1\n",
    "data.loc[(data[\"InSchool\"].isna()) & (data[\"PosWages\"] == 0), \"Idle\"] = 1\n",
    "data.loc[(data[\"InSchool\"] == 0) & (data[\"PosWages\"].isna()), \"Idle\"] = 1\n",
    "\n",
    "# Crime\n",
    "data[\"Convicted\"] = data[[col for col in data.columns if col.startswith('Convicted')]].max(axis=1)\n",
    "data[\"Probation\"] = data[[col for col in data.columns if col.startswith('Probation')]].max(axis=1)\n",
    "data[\"Sentenced\"] = data[[col for col in data.columns if col.startswith('Sentenced')]].max(axis=1)\n",
    "for x in [94, 96, 98, 100, 102, 104]:\n",
    "\tdata[f'Prison{x}'] = np.nan\n",
    "\tdata.loc[data[f'Resid{x}'] == 5, f'Prison{x}'] = 1\n",
    "\tdata.loc[(data[f'Resid{x}'].notna()) & (data[f'Prison{x}'] != 1), f'Prison{x}'] = 0\n",
    "data[\"Prison\"] = data[[f'Prison{x}' for x in [94, 96, 98, 100, 102, 104]]].max(axis=1)\n",
    "data[\"Crime\"] = data[[\"Convicted\", \"Probation\", \"Sentenced\", \"Prison\"]].max(axis=1)\n",
    "\n",
    "# Teen Parenthood\n",
    "data[\"TeenPreg\"] = np.nan\n",
    "data.loc[data[\"Ageat1stBirth\"] < 20, \"TeenPreg\"] = 1\n",
    "data.loc[data[\"Ageat1stBirth\"] >= 20, \"TeenPreg\"] = 0\n",
    "data.loc[(data[\"TeenPreg\"] != 1) & (data[\"YA_NumKids\"].notna()), \"TeenPreg\"] = 0\n",
    "\n",
    "# Poor Health\n",
    "data['HealthReport'] = data[[col for col in data.columns if col.startswith('Health_Report')]].mean(axis=1)\n",
    "data['PoorHealth'] = np.nan\n",
    "data.loc[(data['HealthReport'] < 3) & (data['HealthReport'].notna()), 'PoorHealth'] = 1\n",
    "data.loc[(data['HealthReport'] >= 3) & (data['HealthReport'].notna()), 'PoorHealth'] = 0\n",
    "\n",
    "# dummy outcome\n",
    "for outcome in [\"LD\", \"Repeat\", \"HSGrad\", \"someCollAtt\", \"Idle\", \"Crime\", \"Prison\", \"TeenPreg\", \"PoorHealth\", \"HSGrad_GED\"]:\n",
    "\tdata[outcome + \"_std\"] = (data[outcome] - data[outcome].mean()) / data[outcome].std()\n",
    "\tdata.loc[data[outcome].isna(), outcome + \"_dummy\"] = np.nan\n",
    "for outcome in [\"LD\", \"Repeat\", \"Idle\", \"Crime\", \"Prison\", \"TeenPreg\", \"PoorHealth\"]:\n",
    "\tdata[outcome + \"_std\"] *= -1\n",
    "data[\"Sum_Adult\"] = data[[outcome + \"_std\" for outcome in [\"HSGrad\", \"someCollAtt\", \"Idle\", \"Crime\", \"TeenPreg\", \"PoorHealth\"]]].sum(axis=1)\n",
    "data[\"Sum_Adult\"] = (data[\"Sum_Adult\"] - data[\"Sum_Adult\"].mean()) / data[\"Sum_Adult\"].std()\n",
    "data[\"Noncog\"] = data[[outcome + \"_std\" for outcome in [\"LD\", \"Repeat\"]]].sum(axis=1)\n",
    "data[\"Noncog\"] = (data[\"Noncog\"] - data[\"Noncog\"].mean()) / data[\"Noncog\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All</th>\n",
       "      <th>Black</th>\n",
       "      <th>NonBlack</th>\n",
       "      <th>Male</th>\n",
       "      <th>NonMale</th>\n",
       "      <th>lowAFQT</th>\n",
       "      <th>NonlowAFQT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Repeat</th>\n",
       "      <td>-0.041698</td>\n",
       "      <td>-0.065403</td>\n",
       "      <td>-0.015341</td>\n",
       "      <td>-0.084225</td>\n",
       "      <td>-0.00845</td>\n",
       "      <td>-0.12898</td>\n",
       "      <td>0.005466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repeat se/std</th>\n",
       "      <td>0.051144</td>\n",
       "      <td>0.068219</td>\n",
       "      <td>0.076506</td>\n",
       "      <td>0.059283</td>\n",
       "      <td>0.063029</td>\n",
       "      <td>0.093781</td>\n",
       "      <td>0.061212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Repeat p-value</th>\n",
       "      <td>0.41489529812723724</td>\n",
       "      <td>0.3376997366690061</td>\n",
       "      <td>0.8410714196278483</td>\n",
       "      <td>0.1553956077086122</td>\n",
       "      <td>0.8933512073345268</td>\n",
       "      <td>0.16902693427746218</td>\n",
       "      <td>0.9288439050575381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LD</th>\n",
       "      <td>-0.060985</td>\n",
       "      <td>-0.065517</td>\n",
       "      <td>-0.057809</td>\n",
       "      <td>-0.030295</td>\n",
       "      <td>-0.090296</td>\n",
       "      <td>-0.107898</td>\n",
       "      <td>-0.035039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LD se/std</th>\n",
       "      <td>0.022764</td>\n",
       "      <td>0.029683</td>\n",
       "      <td>0.034735</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>0.024513</td>\n",
       "      <td>0.044998</td>\n",
       "      <td>0.024772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LD p-value</th>\n",
       "      <td>0.007383711738846131</td>\n",
       "      <td>0.027298581253371427</td>\n",
       "      <td>0.09605673382390711</td>\n",
       "      <td>0.317251813235614</td>\n",
       "      <td>0.00022990626430757235</td>\n",
       "      <td>0.016493038611718577</td>\n",
       "      <td>0.15723407868416153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSGrad</th>\n",
       "      <td>0.06645</td>\n",
       "      <td>0.099422</td>\n",
       "      <td>0.03086</td>\n",
       "      <td>0.046076</td>\n",
       "      <td>0.08657</td>\n",
       "      <td>0.164565</td>\n",
       "      <td>0.013432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSGrad se/std</th>\n",
       "      <td>0.034212</td>\n",
       "      <td>0.044208</td>\n",
       "      <td>0.054354</td>\n",
       "      <td>0.044366</td>\n",
       "      <td>0.040335</td>\n",
       "      <td>0.059721</td>\n",
       "      <td>0.040955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSGrad p-value</th>\n",
       "      <td>0.05209942445637254</td>\n",
       "      <td>0.02451328873927494</td>\n",
       "      <td>0.570199901458438</td>\n",
       "      <td>0.29901170052168835</td>\n",
       "      <td>0.03184993287182975</td>\n",
       "      <td>0.005859322238420654</td>\n",
       "      <td>0.7429401920792855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSGrad_GED</th>\n",
       "      <td>0.052719</td>\n",
       "      <td>0.055641</td>\n",
       "      <td>0.050748</td>\n",
       "      <td>0.027466</td>\n",
       "      <td>0.076958</td>\n",
       "      <td>0.126895</td>\n",
       "      <td>0.009958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSGrad_GED se/std</th>\n",
       "      <td>0.035921</td>\n",
       "      <td>0.046017</td>\n",
       "      <td>0.058043</td>\n",
       "      <td>0.048016</td>\n",
       "      <td>0.042125</td>\n",
       "      <td>0.066091</td>\n",
       "      <td>0.041938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSGrad_GED p-value</th>\n",
       "      <td>0.14220346552737897</td>\n",
       "      <td>0.22660926469079423</td>\n",
       "      <td>0.3819480337081591</td>\n",
       "      <td>0.5673096372640989</td>\n",
       "      <td>0.0677192570472408</td>\n",
       "      <td>0.05485777375551435</td>\n",
       "      <td>0.8123184663400003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someCollAtt</th>\n",
       "      <td>0.05827</td>\n",
       "      <td>0.116877</td>\n",
       "      <td>-0.005704</td>\n",
       "      <td>-0.034974</td>\n",
       "      <td>0.147502</td>\n",
       "      <td>-0.005153</td>\n",
       "      <td>0.092087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someCollAtt se/std</th>\n",
       "      <td>0.038811</td>\n",
       "      <td>0.053487</td>\n",
       "      <td>0.054653</td>\n",
       "      <td>0.044445</td>\n",
       "      <td>0.048046</td>\n",
       "      <td>0.056124</td>\n",
       "      <td>0.050144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>someCollAtt p-value</th>\n",
       "      <td>0.13326089871806746</td>\n",
       "      <td>0.028877795435837815</td>\n",
       "      <td>0.9168769336278275</td>\n",
       "      <td>0.43133573676679304</td>\n",
       "      <td>0.002140620989931364</td>\n",
       "      <td>0.9268402431406602</td>\n",
       "      <td>0.06629264105344142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle</th>\n",
       "      <td>-0.087153</td>\n",
       "      <td>-0.062555</td>\n",
       "      <td>-0.118071</td>\n",
       "      <td>-0.109837</td>\n",
       "      <td>-0.063328</td>\n",
       "      <td>-0.116658</td>\n",
       "      <td>-0.072172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle se/std</th>\n",
       "      <td>0.041119</td>\n",
       "      <td>0.058687</td>\n",
       "      <td>0.059675</td>\n",
       "      <td>0.049195</td>\n",
       "      <td>0.048806</td>\n",
       "      <td>0.076087</td>\n",
       "      <td>0.05047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle p-value</th>\n",
       "      <td>0.03404432328501767</td>\n",
       "      <td>0.2864721099754389</td>\n",
       "      <td>0.04786236407844154</td>\n",
       "      <td>0.025570963108194858</td>\n",
       "      <td>0.19444229745188957</td>\n",
       "      <td>0.12522503823980183</td>\n",
       "      <td>0.15271883024637145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crime</th>\n",
       "      <td>-0.003048</td>\n",
       "      <td>-0.006653</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>0.130888</td>\n",
       "      <td>-0.133707</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>-0.008495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crime se/std</th>\n",
       "      <td>0.04146</td>\n",
       "      <td>0.05524</td>\n",
       "      <td>0.061771</td>\n",
       "      <td>0.054613</td>\n",
       "      <td>0.046944</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>0.049496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crime p-value</th>\n",
       "      <td>0.9413939586545744</td>\n",
       "      <td>0.904129633183652</td>\n",
       "      <td>0.9920721238031235</td>\n",
       "      <td>0.016544987199504063</td>\n",
       "      <td>0.004396083314650976</td>\n",
       "      <td>0.9348436517734551</td>\n",
       "      <td>0.8637292239146958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TeenPreg</th>\n",
       "      <td>-0.007973</td>\n",
       "      <td>-0.042582</td>\n",
       "      <td>0.021531</td>\n",
       "      <td>-0.098791</td>\n",
       "      <td>0.083599</td>\n",
       "      <td>-0.015926</td>\n",
       "      <td>-0.00315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TeenPreg se/std</th>\n",
       "      <td>0.039681</td>\n",
       "      <td>0.056168</td>\n",
       "      <td>0.05685</td>\n",
       "      <td>0.050755</td>\n",
       "      <td>0.048757</td>\n",
       "      <td>0.070437</td>\n",
       "      <td>0.047255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TeenPreg p-value</th>\n",
       "      <td>0.8407541153522796</td>\n",
       "      <td>0.44838210498027997</td>\n",
       "      <td>0.704885415760234</td>\n",
       "      <td>0.05160406253408019</td>\n",
       "      <td>0.08641669297309185</td>\n",
       "      <td>0.821123044426091</td>\n",
       "      <td>0.9468485449889102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoorHealth</th>\n",
       "      <td>-0.068461</td>\n",
       "      <td>-0.037561</td>\n",
       "      <td>-0.100327</td>\n",
       "      <td>-0.060797</td>\n",
       "      <td>-0.074464</td>\n",
       "      <td>-0.086584</td>\n",
       "      <td>-0.059279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoorHealth se/std</th>\n",
       "      <td>0.029115</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.046562</td>\n",
       "      <td>0.033276</td>\n",
       "      <td>0.036057</td>\n",
       "      <td>0.051083</td>\n",
       "      <td>0.036242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoorHealth p-value</th>\n",
       "      <td>0.018703641937919412</td>\n",
       "      <td>0.31296888880345863</td>\n",
       "      <td>0.03118553180255446</td>\n",
       "      <td>0.06769268482599641</td>\n",
       "      <td>0.038906349415986675</td>\n",
       "      <td>0.09008404051663053</td>\n",
       "      <td>0.1019090115561095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      All                 Black  \\\n",
       "Repeat                          -0.041698             -0.065403   \n",
       "Repeat se/std                    0.051144              0.068219   \n",
       "Repeat p-value        0.41489529812723724    0.3376997366690061   \n",
       "LD                              -0.060985             -0.065517   \n",
       "LD se/std                        0.022764              0.029683   \n",
       "LD p-value           0.007383711738846131  0.027298581253371427   \n",
       "HSGrad                            0.06645              0.099422   \n",
       "HSGrad se/std                    0.034212              0.044208   \n",
       "HSGrad p-value        0.05209942445637254   0.02451328873927494   \n",
       "HSGrad_GED                       0.052719              0.055641   \n",
       "HSGrad_GED se/std                0.035921              0.046017   \n",
       "HSGrad_GED p-value    0.14220346552737897   0.22660926469079423   \n",
       "someCollAtt                       0.05827              0.116877   \n",
       "someCollAtt se/std               0.038811              0.053487   \n",
       "someCollAtt p-value   0.13326089871806746  0.028877795435837815   \n",
       "Idle                            -0.087153             -0.062555   \n",
       "Idle se/std                      0.041119              0.058687   \n",
       "Idle p-value          0.03404432328501767    0.2864721099754389   \n",
       "Crime                           -0.003048             -0.006653   \n",
       "Crime se/std                      0.04146               0.05524   \n",
       "Crime p-value          0.9413939586545744     0.904129633183652   \n",
       "TeenPreg                        -0.007973             -0.042582   \n",
       "TeenPreg se/std                  0.039681              0.056168   \n",
       "TeenPreg p-value       0.8407541153522796   0.44838210498027997   \n",
       "PoorHealth                      -0.068461             -0.037561   \n",
       "PoorHealth se/std                0.029115              0.037226   \n",
       "PoorHealth p-value   0.018703641937919412   0.31296888880345863   \n",
       "\n",
       "                                NonBlack                  Male  \\\n",
       "Repeat                         -0.015341             -0.084225   \n",
       "Repeat se/std                   0.076506              0.059283   \n",
       "Repeat p-value        0.8410714196278483    0.1553956077086122   \n",
       "LD                             -0.057809             -0.030295   \n",
       "LD se/std                       0.034735              0.030291   \n",
       "LD p-value           0.09605673382390711     0.317251813235614   \n",
       "HSGrad                           0.03086              0.046076   \n",
       "HSGrad se/std                   0.054354              0.044366   \n",
       "HSGrad p-value         0.570199901458438   0.29901170052168835   \n",
       "HSGrad_GED                      0.050748              0.027466   \n",
       "HSGrad_GED se/std               0.058043              0.048016   \n",
       "HSGrad_GED p-value    0.3819480337081591    0.5673096372640989   \n",
       "someCollAtt                    -0.005704             -0.034974   \n",
       "someCollAtt se/std              0.054653              0.044445   \n",
       "someCollAtt p-value   0.9168769336278275   0.43133573676679304   \n",
       "Idle                           -0.118071             -0.109837   \n",
       "Idle se/std                     0.059675              0.049195   \n",
       "Idle p-value         0.04786236407844154  0.025570963108194858   \n",
       "Crime                          -0.000614              0.130888   \n",
       "Crime se/std                    0.061771              0.054613   \n",
       "Crime p-value         0.9920721238031235  0.016544987199504063   \n",
       "TeenPreg                        0.021531             -0.098791   \n",
       "TeenPreg se/std                  0.05685              0.050755   \n",
       "TeenPreg p-value       0.704885415760234   0.05160406253408019   \n",
       "PoorHealth                     -0.100327             -0.060797   \n",
       "PoorHealth se/std               0.046562              0.033276   \n",
       "PoorHealth p-value   0.03118553180255446   0.06769268482599641   \n",
       "\n",
       "                                    NonMale               lowAFQT  \\\n",
       "Repeat                             -0.00845              -0.12898   \n",
       "Repeat se/std                      0.063029              0.093781   \n",
       "Repeat p-value           0.8933512073345268   0.16902693427746218   \n",
       "LD                                -0.090296             -0.107898   \n",
       "LD se/std                          0.024513              0.044998   \n",
       "LD p-value           0.00022990626430757235  0.016493038611718577   \n",
       "HSGrad                              0.08657              0.164565   \n",
       "HSGrad se/std                      0.040335              0.059721   \n",
       "HSGrad p-value          0.03184993287182975  0.005859322238420654   \n",
       "HSGrad_GED                         0.076958              0.126895   \n",
       "HSGrad_GED se/std                  0.042125              0.066091   \n",
       "HSGrad_GED p-value       0.0677192570472408   0.05485777375551435   \n",
       "someCollAtt                        0.147502             -0.005153   \n",
       "someCollAtt se/std                 0.048046              0.056124   \n",
       "someCollAtt p-value    0.002140620989931364    0.9268402431406602   \n",
       "Idle                              -0.063328             -0.116658   \n",
       "Idle se/std                        0.048806              0.076087   \n",
       "Idle p-value            0.19444229745188957   0.12522503823980183   \n",
       "Crime                             -0.133707              0.005989   \n",
       "Crime se/std                       0.046944              0.073252   \n",
       "Crime p-value          0.004396083314650976    0.9348436517734551   \n",
       "TeenPreg                           0.083599             -0.015926   \n",
       "TeenPreg se/std                    0.048757              0.070437   \n",
       "TeenPreg p-value        0.08641669297309185     0.821123044426091   \n",
       "PoorHealth                        -0.074464             -0.086584   \n",
       "PoorHealth se/std                  0.036057              0.051083   \n",
       "PoorHealth p-value     0.038906349415986675   0.09008404051663053   \n",
       "\n",
       "                              NonlowAFQT  \n",
       "Repeat                          0.005466  \n",
       "Repeat se/std                   0.061212  \n",
       "Repeat p-value        0.9288439050575381  \n",
       "LD                             -0.035039  \n",
       "LD se/std                       0.024772  \n",
       "LD p-value           0.15723407868416153  \n",
       "HSGrad                          0.013432  \n",
       "HSGrad se/std                   0.040955  \n",
       "HSGrad p-value        0.7429401920792855  \n",
       "HSGrad_GED                      0.009958  \n",
       "HSGrad_GED se/std               0.041938  \n",
       "HSGrad_GED p-value    0.8123184663400003  \n",
       "someCollAtt                     0.092087  \n",
       "someCollAtt se/std              0.050144  \n",
       "someCollAtt p-value  0.06629264105344142  \n",
       "Idle                           -0.072172  \n",
       "Idle se/std                      0.05047  \n",
       "Idle p-value         0.15271883024637145  \n",
       "Crime                          -0.008495  \n",
       "Crime se/std                    0.049496  \n",
       "Crime p-value         0.8637292239146958  \n",
       "TeenPreg                        -0.00315  \n",
       "TeenPreg se/std                 0.047255  \n",
       "TeenPreg p-value      0.9468485449889102  \n",
       "PoorHealth                     -0.059279  \n",
       "PoorHealth se/std               0.036242  \n",
       "PoorHealth p-value    0.1019090115561095  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table 5\n",
    "column_names = [\"All\", \"Black\", \"NonBlack\", \"Male\", \"NonMale\", \"lowAFQT\", \"NonlowAFQT\"]\n",
    "group_names = [\"Repeat\", \"LD\", \"HSGrad\", \"HSGrad_GED\", \"someCollAtt\", \"Idle\", \"Crime\", \"TeenPreg\", \"PoorHealth\"]\n",
    "row_names = []\n",
    "for group in group_names:\n",
    "    row_names.append(group)\n",
    "    row_names.append(group + \" se/std\")\n",
    "    row_names.append(group + \" p-value\")\n",
    "table5 = pd.DataFrame(index=row_names, columns=column_names)\n",
    "covariates = lambda group: [\"headstart_fixedeffect_indicator_90\", \"preschool_fixedeffect_indicator_90\", \"C(cleaned_age_yr_104)\"] +\\\n",
    "            covariates_to_impute + [cov + \"_missing\" for cov in covariates_to_impute] + [\"C(MotherID)\"] + ([\"Male\"] if group != \"Male\" else [])\n",
    "group_covariates = lambda group: [f\"HS_{group}\", f\"HS_Non{group}\", f\"Pre_{group}\", f\"Pre_Non{group}\"] + covariates(group)[2:]\n",
    "for outcome in group_names:\n",
    "    mask = data[outcome].notna()\n",
    "    model = smf.ols(f\"{outcome} ~ \" + \" + \".join(covariates(group)), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "    table5.loc[outcome, \"All\"] = model.params[\"headstart_fixedeffect_indicator_90\"]\n",
    "    table5.loc[outcome + \" se/std\", \"All\"] = model.bse[\"headstart_fixedeffect_indicator_90\"]\n",
    "    table5.loc[outcome + \" p-value\", \"All\"] = model.t_test(\"headstart_fixedeffect_indicator_90 = 0\").pvalue\n",
    "\n",
    "    for group in [\"Black\", \"Male\", \"lowAFQT\"]:\n",
    "        model = smf.ols(f\"{outcome} ~ \" + \" + \".join(group_covariates(group)), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "        table5.loc[outcome, group] = model.params[f\"HS_{group}\"]\n",
    "        table5.loc[outcome + \" se/std\", group] = model.bse[f\"HS_{group}\"]\n",
    "        table5.loc[outcome + \" p-value\", group] = model.t_test(f\"HS_{group} = 0\").pvalue\n",
    "        table5.loc[outcome, f\"Non{group}\"] = model.params[f\"HS_Non{group}\"]\n",
    "        table5.loc[outcome + \" se/std\", f\"Non{group}\"] = model.bse[f\"HS_Non{group}\"]\n",
    "        table5.loc[outcome + \" p-value\", f\"Non{group}\"] = model.t_test(f\"HS_Non{group} = 0\").pvalue\n",
    "\n",
    "table5.to_csv(\"table5.csv\")\n",
    "table5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Scores, 5-6</th>\n",
       "      <th>Test Scores, 7-10</th>\n",
       "      <th>Test Scores, 11-14</th>\n",
       "      <th>Test Scores, 5-14</th>\n",
       "      <th>Nontest score, 5-14</th>\n",
       "      <th>Long term, 19+</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Head Start, overall</th>\n",
       "      <td>0.146343</td>\n",
       "      <td>0.127851</td>\n",
       "      <td>0.050266</td>\n",
       "      <td>0.096006</td>\n",
       "      <td>0.20832</td>\n",
       "      <td>0.237244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, overall, se</th>\n",
       "      <td>0.089245</td>\n",
       "      <td>0.063647</td>\n",
       "      <td>0.065064</td>\n",
       "      <td>0.060271</td>\n",
       "      <td>0.089733</td>\n",
       "      <td>0.081203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, overall, p-value</th>\n",
       "      <td>0.10104822728752048</td>\n",
       "      <td>0.04456165774594422</td>\n",
       "      <td>0.43978184138694987</td>\n",
       "      <td>0.11118319114094476</td>\n",
       "      <td>0.020256867249719545</td>\n",
       "      <td>0.003482059284241549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other preschool, overall</th>\n",
       "      <td>-0.034753</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>-0.025147</td>\n",
       "      <td>-0.002532</td>\n",
       "      <td>0.095127</td>\n",
       "      <td>0.072341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other preschool, overall, se</th>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.066156</td>\n",
       "      <td>0.071551</td>\n",
       "      <td>0.063547</td>\n",
       "      <td>0.097689</td>\n",
       "      <td>0.077079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other preschool, overall, p-value</th>\n",
       "      <td>0.688422618589044</td>\n",
       "      <td>0.6099846080937569</td>\n",
       "      <td>0.7252478089046317</td>\n",
       "      <td>0.968213242584602</td>\n",
       "      <td>0.33017179422968834</td>\n",
       "      <td>0.3479684085469896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p (HS = preschool), overall</th>\n",
       "      <td>0.061413</td>\n",
       "      <td>0.228421</td>\n",
       "      <td>0.342406</td>\n",
       "      <td>0.189426</td>\n",
       "      <td>0.320812</td>\n",
       "      <td>0.10198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, Black</th>\n",
       "      <td>0.251355</td>\n",
       "      <td>0.143366</td>\n",
       "      <td>-0.00146</td>\n",
       "      <td>0.095295</td>\n",
       "      <td>0.239401</td>\n",
       "      <td>0.272902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, Black, se</th>\n",
       "      <td>0.10908</td>\n",
       "      <td>0.079076</td>\n",
       "      <td>0.078149</td>\n",
       "      <td>0.075258</td>\n",
       "      <td>0.121528</td>\n",
       "      <td>0.114016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, Black, p-value</th>\n",
       "      <td>0.021205074038067567</td>\n",
       "      <td>0.06982915895486999</td>\n",
       "      <td>0.9850912790580943</td>\n",
       "      <td>0.20542536709141146</td>\n",
       "      <td>0.04884709668394021</td>\n",
       "      <td>0.016686215494564737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, NonBlack</th>\n",
       "      <td>-0.001194</td>\n",
       "      <td>0.108117</td>\n",
       "      <td>0.126508</td>\n",
       "      <td>0.103357</td>\n",
       "      <td>0.182344</td>\n",
       "      <td>0.204282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, NonBlack, se</th>\n",
       "      <td>0.126998</td>\n",
       "      <td>0.100017</td>\n",
       "      <td>0.104482</td>\n",
       "      <td>0.097143</td>\n",
       "      <td>0.135129</td>\n",
       "      <td>0.115047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, NonBlack, p-value</th>\n",
       "      <td>0.9925016975239058</td>\n",
       "      <td>0.2797004252854578</td>\n",
       "      <td>0.22596446930672276</td>\n",
       "      <td>0.28734402353606936</td>\n",
       "      <td>0.1772061397656779</td>\n",
       "      <td>0.07579232413408499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p (Black=NonBlack), HS</th>\n",
       "      <td>0.096984</td>\n",
       "      <td>0.77862</td>\n",
       "      <td>0.317857</td>\n",
       "      <td>0.947429</td>\n",
       "      <td>0.751217</td>\n",
       "      <td>0.669587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, Male</th>\n",
       "      <td>0.072348</td>\n",
       "      <td>0.115893</td>\n",
       "      <td>0.051723</td>\n",
       "      <td>0.079411</td>\n",
       "      <td>0.150325</td>\n",
       "      <td>0.157035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, Male, se</th>\n",
       "      <td>0.103392</td>\n",
       "      <td>0.074608</td>\n",
       "      <td>0.075283</td>\n",
       "      <td>0.069181</td>\n",
       "      <td>0.121472</td>\n",
       "      <td>0.09818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, Male, p-value</th>\n",
       "      <td>0.4840863594486653</td>\n",
       "      <td>0.12033676953811571</td>\n",
       "      <td>0.492054315169035</td>\n",
       "      <td>0.2510219412390986</td>\n",
       "      <td>0.21589141731420058</td>\n",
       "      <td>0.10971925916080588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, NonMale</th>\n",
       "      <td>0.22236</td>\n",
       "      <td>0.14568</td>\n",
       "      <td>0.053352</td>\n",
       "      <td>0.115084</td>\n",
       "      <td>0.268504</td>\n",
       "      <td>0.308697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, NonMale, se</th>\n",
       "      <td>0.110333</td>\n",
       "      <td>0.077226</td>\n",
       "      <td>0.079097</td>\n",
       "      <td>0.073246</td>\n",
       "      <td>0.103731</td>\n",
       "      <td>0.094744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, NonMale, p-value</th>\n",
       "      <td>0.04386731108158828</td>\n",
       "      <td>0.059241391150804214</td>\n",
       "      <td>0.4999879873883909</td>\n",
       "      <td>0.11613756057246864</td>\n",
       "      <td>0.00964070912041179</td>\n",
       "      <td>0.0011211982047706058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p (Male=NonMale), HS</th>\n",
       "      <td>0.200721</td>\n",
       "      <td>0.716954</td>\n",
       "      <td>0.984306</td>\n",
       "      <td>0.638164</td>\n",
       "      <td>0.378926</td>\n",
       "      <td>0.147227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, lowAFQT</th>\n",
       "      <td>0.182461</td>\n",
       "      <td>0.039937</td>\n",
       "      <td>-0.052866</td>\n",
       "      <td>0.013155</td>\n",
       "      <td>0.441447</td>\n",
       "      <td>0.307721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, lowAFQT, se</th>\n",
       "      <td>0.136924</td>\n",
       "      <td>0.100159</td>\n",
       "      <td>0.102697</td>\n",
       "      <td>0.097024</td>\n",
       "      <td>0.172994</td>\n",
       "      <td>0.131533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, lowAFQT, p-value</th>\n",
       "      <td>0.18267390681350482</td>\n",
       "      <td>0.6900888674240584</td>\n",
       "      <td>0.606705535048625</td>\n",
       "      <td>0.8921523463838559</td>\n",
       "      <td>0.010716602245059699</td>\n",
       "      <td>0.019310192790968326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, NonlowAFQT</th>\n",
       "      <td>0.139393</td>\n",
       "      <td>0.175229</td>\n",
       "      <td>0.107605</td>\n",
       "      <td>0.140859</td>\n",
       "      <td>0.080597</td>\n",
       "      <td>0.199718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, NonlowAFQT, se</th>\n",
       "      <td>0.10478</td>\n",
       "      <td>0.07933</td>\n",
       "      <td>0.081536</td>\n",
       "      <td>0.076019</td>\n",
       "      <td>0.103608</td>\n",
       "      <td>0.102668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Head Start, NonlowAFQT, p-value</th>\n",
       "      <td>0.18340803302530373</td>\n",
       "      <td>0.02718513653935129</td>\n",
       "      <td>0.18692497955248555</td>\n",
       "      <td>0.06388979865973873</td>\n",
       "      <td>0.43662810676959585</td>\n",
       "      <td>0.051742507427555436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p (lowAFQT=NonlowAFQT), HS</th>\n",
       "      <td>0.787071</td>\n",
       "      <td>0.282184</td>\n",
       "      <td>0.216554</td>\n",
       "      <td>0.301135</td>\n",
       "      <td>0.077387</td>\n",
       "      <td>0.515956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Test Scores, 5-6     Test Scores, 7-10  \\\n",
       "Head Start, overall                            0.146343              0.127851   \n",
       "Head Start, overall, se                        0.089245              0.063647   \n",
       "Head Start, overall, p-value        0.10104822728752048   0.04456165774594422   \n",
       "Other preschool, overall                      -0.034753              0.033746   \n",
       "Other preschool, overall, se                   0.086667              0.066156   \n",
       "Other preschool, overall, p-value     0.688422618589044    0.6099846080937569   \n",
       "p (HS = preschool), overall                    0.061413              0.228421   \n",
       "Head Start, Black                              0.251355              0.143366   \n",
       "Head Start, Black, se                           0.10908              0.079076   \n",
       "Head Start, Black, p-value         0.021205074038067567   0.06982915895486999   \n",
       "Head Start, NonBlack                          -0.001194              0.108117   \n",
       "Head Start, NonBlack, se                       0.126998              0.100017   \n",
       "Head Start, NonBlack, p-value        0.9925016975239058    0.2797004252854578   \n",
       "p (Black=NonBlack), HS                         0.096984               0.77862   \n",
       "Head Start, Male                               0.072348              0.115893   \n",
       "Head Start, Male, se                           0.103392              0.074608   \n",
       "Head Start, Male, p-value            0.4840863594486653   0.12033676953811571   \n",
       "Head Start, NonMale                             0.22236               0.14568   \n",
       "Head Start, NonMale, se                        0.110333              0.077226   \n",
       "Head Start, NonMale, p-value        0.04386731108158828  0.059241391150804214   \n",
       "p (Male=NonMale), HS                           0.200721              0.716954   \n",
       "Head Start, lowAFQT                            0.182461              0.039937   \n",
       "Head Start, lowAFQT, se                        0.136924              0.100159   \n",
       "Head Start, lowAFQT, p-value        0.18267390681350482    0.6900888674240584   \n",
       "Head Start, NonlowAFQT                         0.139393              0.175229   \n",
       "Head Start, NonlowAFQT, se                      0.10478               0.07933   \n",
       "Head Start, NonlowAFQT, p-value     0.18340803302530373   0.02718513653935129   \n",
       "p (lowAFQT=NonlowAFQT), HS                     0.787071              0.282184   \n",
       "\n",
       "                                    Test Scores, 11-14    Test Scores, 5-14  \\\n",
       "Head Start, overall                           0.050266             0.096006   \n",
       "Head Start, overall, se                       0.065064             0.060271   \n",
       "Head Start, overall, p-value       0.43978184138694987  0.11118319114094476   \n",
       "Other preschool, overall                     -0.025147            -0.002532   \n",
       "Other preschool, overall, se                  0.071551             0.063547   \n",
       "Other preschool, overall, p-value   0.7252478089046317    0.968213242584602   \n",
       "p (HS = preschool), overall                   0.342406             0.189426   \n",
       "Head Start, Black                             -0.00146             0.095295   \n",
       "Head Start, Black, se                         0.078149             0.075258   \n",
       "Head Start, Black, p-value          0.9850912790580943  0.20542536709141146   \n",
       "Head Start, NonBlack                          0.126508             0.103357   \n",
       "Head Start, NonBlack, se                      0.104482             0.097143   \n",
       "Head Start, NonBlack, p-value      0.22596446930672276  0.28734402353606936   \n",
       "p (Black=NonBlack), HS                        0.317857             0.947429   \n",
       "Head Start, Male                              0.051723             0.079411   \n",
       "Head Start, Male, se                          0.075283             0.069181   \n",
       "Head Start, Male, p-value            0.492054315169035   0.2510219412390986   \n",
       "Head Start, NonMale                           0.053352             0.115084   \n",
       "Head Start, NonMale, se                       0.079097             0.073246   \n",
       "Head Start, NonMale, p-value        0.4999879873883909  0.11613756057246864   \n",
       "p (Male=NonMale), HS                          0.984306             0.638164   \n",
       "Head Start, lowAFQT                          -0.052866             0.013155   \n",
       "Head Start, lowAFQT, se                       0.102697             0.097024   \n",
       "Head Start, lowAFQT, p-value         0.606705535048625   0.8921523463838559   \n",
       "Head Start, NonlowAFQT                        0.107605             0.140859   \n",
       "Head Start, NonlowAFQT, se                    0.081536             0.076019   \n",
       "Head Start, NonlowAFQT, p-value    0.18692497955248555  0.06388979865973873   \n",
       "p (lowAFQT=NonlowAFQT), HS                    0.216554             0.301135   \n",
       "\n",
       "                                    Nontest score, 5-14         Long term, 19+  \n",
       "Head Start, overall                             0.20832               0.237244  \n",
       "Head Start, overall, se                        0.089733               0.081203  \n",
       "Head Start, overall, p-value       0.020256867249719545   0.003482059284241549  \n",
       "Other preschool, overall                       0.095127               0.072341  \n",
       "Other preschool, overall, se                   0.097689               0.077079  \n",
       "Other preschool, overall, p-value   0.33017179422968834     0.3479684085469896  \n",
       "p (HS = preschool), overall                    0.320812                0.10198  \n",
       "Head Start, Black                              0.239401               0.272902  \n",
       "Head Start, Black, se                          0.121528               0.114016  \n",
       "Head Start, Black, p-value          0.04884709668394021   0.016686215494564737  \n",
       "Head Start, NonBlack                           0.182344               0.204282  \n",
       "Head Start, NonBlack, se                       0.135129               0.115047  \n",
       "Head Start, NonBlack, p-value        0.1772061397656779    0.07579232413408499  \n",
       "p (Black=NonBlack), HS                         0.751217               0.669587  \n",
       "Head Start, Male                               0.150325               0.157035  \n",
       "Head Start, Male, se                           0.121472                0.09818  \n",
       "Head Start, Male, p-value           0.21589141731420058    0.10971925916080588  \n",
       "Head Start, NonMale                            0.268504               0.308697  \n",
       "Head Start, NonMale, se                        0.103731               0.094744  \n",
       "Head Start, NonMale, p-value        0.00964070912041179  0.0011211982047706058  \n",
       "p (Male=NonMale), HS                           0.378926               0.147227  \n",
       "Head Start, lowAFQT                            0.441447               0.307721  \n",
       "Head Start, lowAFQT, se                        0.172994               0.131533  \n",
       "Head Start, lowAFQT, p-value       0.010716602245059699   0.019310192790968326  \n",
       "Head Start, NonlowAFQT                         0.080597               0.199718  \n",
       "Head Start, NonlowAFQT, se                     0.103608               0.102668  \n",
       "Head Start, NonlowAFQT, p-value     0.43662810676959585   0.051742507427555436  \n",
       "p (lowAFQT=NonlowAFQT), HS                     0.077387               0.515956  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table 4\n",
    "column_names = [\"Test Scores, 5-6\", \"Test Scores, 7-10\", \"Test Scores, 11-14\", \"Test Scores, 5-14\", \n",
    "                \"Nontest score, 5-14\", \"Long term, 19+\"]\n",
    "row_names = [\"Head Start, overall\", \"Head Start, overall, se\", \"Head Start, overall, p-value\", \"Other preschool, overall\", \"Other preschool, overall, se\", \"Other preschool, overall, p-value\", \"p (HS = preschool), overall\",\n",
    "             \"Head Start, Black\", \"Head Start, Black, se\", \"Head Start, Black, p-value\", \"Head Start, NonBlack\", \"Head Start, NonBlack, se\", \"Head Start, NonBlack, p-value\", \"p (Black=NonBlack), HS\",\n",
    "             \"Head Start, Male\", \"Head Start, Male, se\", \"Head Start, Male, p-value\", \"Head Start, NonMale\", \"Head Start, NonMale, se\", \"Head Start, NonMale, p-value\", \"p (Male=NonMale), HS\",\n",
    "             \"Head Start, lowAFQT\", \"Head Start, lowAFQT, se\", \"Head Start, lowAFQT, p-value\", \"Head Start, NonlowAFQT\", \"Head Start, NonlowAFQT, se\", \"Head Start, NonlowAFQT, p-value\", \"p (lowAFQT=NonlowAFQT), HS\"]\n",
    "table4 = pd.DataFrame(index=row_names, columns=column_names)\n",
    "\n",
    "baseline_covariates = [\"HS_5to6\", \"HS_7to10\", \"HS_11to14\", \"Pre_5to6\", \"Pre_7to10\", \"Pre_11to14\", \"Male\", \"C(AgeTest_Yr)\", \"C(Year)\"]\n",
    "overall_baseline_covariates = [\"headstart_fixedeffect_indicator_90\", \"preschool_fixedeffect_indicator_90\", \"Male\", \"C(AgeTest_Yr)\", \"C(Year)\"]\n",
    "standard_covariates = covariates_to_impute + [cov + \"_missing\" for cov in covariates_to_impute]\n",
    "family_fixed_effects = [\"C(MotherID)\"]\n",
    "\n",
    "# test scores\n",
    "# overall\n",
    "mask = data['Test_std'].notna()\n",
    "model = smf.ols(\"Test_std ~ \" + \" + \".join(baseline_covariates + standard_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "table4.loc[\"Head Start, overall\", \"Test Scores, 5-6\"] = model.params[\"HS_5to6\"]\n",
    "table4.loc[\"Head Start, overall, se\", \"Test Scores, 5-6\"] = model.bse[\"HS_5to6\"]\n",
    "table4.loc[\"Head Start, overall, p-value\", \"Test Scores, 5-6\"] = model.t_test(\"HS_5to6 = 0\").pvalue\n",
    "table4.loc[\"Head Start, overall\", \"Test Scores, 7-10\"] = model.params[\"HS_7to10\"]\n",
    "table4.loc[\"Head Start, overall, se\", \"Test Scores, 7-10\"] = model.bse[\"HS_7to10\"]\n",
    "table4.loc[\"Head Start, overall, p-value\", \"Test Scores, 7-10\"] = model.t_test(\"HS_7to10 = 0\").pvalue\n",
    "table4.loc[\"Head Start, overall\", \"Test Scores, 11-14\"] = model.params[\"HS_11to14\"]\n",
    "table4.loc[\"Head Start, overall, se\", \"Test Scores, 11-14\"] = model.bse[\"HS_11to14\"]\n",
    "table4.loc[\"Head Start, overall, p-value\", \"Test Scores, 11-14\"] = model.t_test(\"HS_11to14 = 0\").pvalue\n",
    "table4.loc[\"Other preschool, overall\", \"Test Scores, 5-6\"] = model.params[\"Pre_5to6\"]\n",
    "table4.loc[\"Other preschool, overall, se\", \"Test Scores, 5-6\"] = model.bse[\"Pre_5to6\"]\n",
    "table4.loc[\"Other preschool, overall, p-value\", \"Test Scores, 5-6\"] = model.t_test(\"Pre_5to6 = 0\").pvalue\n",
    "table4.loc[\"Other preschool, overall\", \"Test Scores, 7-10\"] = model.params[\"Pre_7to10\"]\n",
    "table4.loc[\"Other preschool, overall, se\", \"Test Scores, 7-10\"] = model.bse[\"Pre_7to10\"]\n",
    "table4.loc[\"Other preschool, overall, p-value\", \"Test Scores, 7-10\"] = model.t_test(\"Pre_7to10 = 0\").pvalue\n",
    "table4.loc[\"Other preschool, overall\", \"Test Scores, 11-14\"] = model.params[\"Pre_11to14\"]\n",
    "table4.loc[\"Other preschool, overall, se\", \"Test Scores, 11-14\"] = model.bse[\"Pre_11to14\"]\n",
    "table4.loc[\"Other preschool, overall, p-value\", \"Test Scores, 11-14\"] = model.t_test(\"Pre_11to14 = 0\").pvalue\n",
    "\n",
    "table4.loc[\"p (HS = preschool), overall\", \"Test Scores, 5-6\"] = model.f_test(\"HS_5to6 = Pre_5to6\").pvalue\n",
    "table4.loc[\"p (HS = preschool), overall\", \"Test Scores, 7-10\"] = model.f_test(\"HS_7to10 = Pre_7to10\").pvalue\n",
    "table4.loc[\"p (HS = preschool), overall\", \"Test Scores, 11-14\"] = model.f_test(\"HS_11to14 = Pre_11to14\").pvalue\n",
    "\n",
    "\n",
    "model = smf.ols(\"Test_std ~ \" + \" + \".join(overall_baseline_covariates + standard_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "table4.loc[\"Head Start, overall\", \"Test Scores, 5-14\"] = model.params[\"headstart_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Head Start, overall, se\", \"Test Scores, 5-14\"] = model.bse[\"headstart_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Head Start, overall, p-value\", \"Test Scores, 5-14\"] = model.t_test(\"headstart_fixedeffect_indicator_90 = 0\").pvalue\n",
    "table4.loc[\"Other preschool, overall\", \"Test Scores, 5-14\"] = model.params[\"preschool_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Other preschool, overall, se\", \"Test Scores, 5-14\"] = model.bse[\"preschool_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Other preschool, overall, p-value\", \"Test Scores, 5-14\"] = model.t_test(\"preschool_fixedeffect_indicator_90 = 0\").pvalue\n",
    "\n",
    "table4.loc[\"p (HS = preschool), overall\", \"Test Scores, 5-14\"] = model.f_test(\"headstart_fixedeffect_indicator_90 = preschool_fixedeffect_indicator_90\").pvalue\n",
    "\n",
    "# by subgroup\n",
    "for name in [\"Male\", \"Black\", \"lowAFQT\"]:\n",
    "    _baseline_covariates = [f\"HS_{name}_5to6\", f\"HS_{name}_7to10\", f\"HS_{name}_11to14\", \n",
    "                            f\"Pre_{name}_5to6\", f\"Pre_{name}_7to10\", f\"Pre_{name}_11to14\",\n",
    "                            f\"HS_Non{name}_5to6\", f\"HS_Non{name}_7to10\", f\"HS_Non{name}_11to14\",\n",
    "                            f\"Pre_Non{name}_5to6\", f\"Pre_Non{name}_7to10\", f\"Pre_Non{name}_11to14\"]\n",
    "    _baseline_covariates += [\"C(AgeTest_Yr)\", \"C(Year)\"]\n",
    "    if name != \"Male\":\n",
    "        _baseline_covariates += [\"Male\"]\n",
    "    model = smf.ols(\"Test_std ~ \" + \" + \".join(_baseline_covariates + standard_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "    table4.loc[f\"Head Start, {name}\", \"Test Scores, 5-6\"] = model.params[f\"HS_{name}_5to6\"]\n",
    "    table4.loc[f\"Head Start, {name}, se\", \"Test Scores, 5-6\"] = model.bse[f\"HS_{name}_5to6\"]\n",
    "    table4.loc[f\"Head Start, {name}, p-value\", \"Test Scores, 5-6\"] = model.t_test(f\"HS_{name}_5to6 = 0\").pvalue\n",
    "    table4.loc[f\"Head Start, {name}\", \"Test Scores, 7-10\"] = model.params[f\"HS_{name}_7to10\"]\n",
    "    table4.loc[f\"Head Start, {name}, se\", \"Test Scores, 7-10\"] = model.bse[f\"HS_{name}_7to10\"]\n",
    "    table4.loc[f\"Head Start, {name}, p-value\", \"Test Scores, 7-10\"] = model.t_test(f\"HS_{name}_7to10 = 0\").pvalue\n",
    "    table4.loc[f\"Head Start, {name}\", \"Test Scores, 11-14\"] = model.params[f\"HS_{name}_11to14\"]\n",
    "    table4.loc[f\"Head Start, {name}, se\", \"Test Scores, 11-14\"] = model.bse[f\"HS_{name}_11to14\"]\n",
    "    table4.loc[f\"Head Start, {name}, p-value\", \"Test Scores, 11-14\"] = model.t_test(f\"HS_{name}_11to14 = 0\").pvalue\n",
    "    table4.loc[f\"Head Start, Non{name}\", \"Test Scores, 5-6\"] = model.params[f\"HS_Non{name}_5to6\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, se\", \"Test Scores, 5-6\"] = model.bse[f\"HS_Non{name}_5to6\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, p-value\", \"Test Scores, 5-6\"] = model.t_test(f\"HS_Non{name}_5to6 = 0\").pvalue\n",
    "    table4.loc[f\"Head Start, Non{name}\", \"Test Scores, 7-10\"] = model.params[f\"HS_Non{name}_7to10\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, se\", \"Test Scores, 7-10\"] = model.bse[f\"HS_Non{name}_7to10\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, p-value\", \"Test Scores, 7-10\"] = model.t_test(f\"HS_Non{name}_7to10 = 0\").pvalue\n",
    "    table4.loc[f\"Head Start, Non{name}\", \"Test Scores, 11-14\"] = model.params[f\"HS_Non{name}_11to14\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, se\", \"Test Scores, 11-14\"] = model.bse[f\"HS_Non{name}_11to14\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, p-value\", \"Test Scores, 11-14\"] = model.t_test(f\"HS_Non{name}_11to14 = 0\").pvalue\n",
    "\n",
    "    table4.loc[f\"p ({name}=Non{name}), HS\", \"Test Scores, 5-6\"] = model.f_test(f\"HS_{name}_5to6 = HS_Non{name}_5to6\").pvalue\n",
    "    table4.loc[f\"p ({name}=Non{name}), HS\", \"Test Scores, 7-10\"] = model.f_test(f\"HS_{name}_7to10 = HS_Non{name}_7to10\").pvalue\n",
    "    table4.loc[f\"p ({name}=Non{name}), HS\", \"Test Scores, 11-14\"] = model.f_test(f\"HS_{name}_11to14 = HS_Non{name}_11to14\").pvalue\n",
    "\n",
    "\n",
    "    _overall_baseline_covariates = [f\"HS_{name}\",\n",
    "                                    f\"Pre_{name}\",\n",
    "                                    f\"HS_Non{name}\",\n",
    "                                    f\"Pre_Non{name}\",]\n",
    "    _overall_baseline_covariates += [\"C(AgeTest_Yr)\", \"C(Year)\"]\n",
    "    if name != \"Male\":\n",
    "        _overall_baseline_covariates += [\"Male\"]\n",
    "    model = smf.ols(\"Test_std ~ \" + \" + \".join(_overall_baseline_covariates + standard_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "    table4.loc[f\"Head Start, {name}\", \"Test Scores, 5-14\"] = model.params[f\"HS_{name}\"]\n",
    "    table4.loc[f\"Head Start, {name}, se\", \"Test Scores, 5-14\"] = model.bse[f\"HS_{name}\"]\n",
    "    table4.loc[f\"Head Start, {name}, p-value\", \"Test Scores, 5-14\"] = model.t_test(f\"HS_{name} = 0\").pvalue\n",
    "    table4.loc[f\"Head Start, Non{name}\", \"Test Scores, 5-14\"] = model.params[f\"HS_Non{name}\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, se\", \"Test Scores, 5-14\"] = model.bse[f\"HS_Non{name}\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, p-value\", \"Test Scores, 5-14\"] = model.t_test(f\"HS_Non{name} = 0\").pvalue\n",
    "\n",
    "    table4.loc[f\"p ({name}=Non{name}), HS\", \"Test Scores, 5-14\"] = model.f_test(f\"HS_{name} = HS_Non{name}\").pvalue\n",
    "\n",
    "\n",
    "# noncognitive outcomes, 5-14\n",
    "mask = data['Noncog'].notna()\n",
    "model = smf.ols(\"Noncog ~ \" + \" + \".join(overall_baseline_covariates + standard_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "table4.loc[\"Head Start, overall\", \"Nontest score, 5-14\"] = model.params[\"headstart_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Head Start, overall, se\", \"Nontest score, 5-14\"] = model.bse[\"headstart_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Head Start, overall, p-value\", \"Nontest score, 5-14\"] = model.t_test(\"headstart_fixedeffect_indicator_90 = 0\").pvalue\n",
    "table4.loc[\"Other preschool, overall\", \"Nontest score, 5-14\"] = model.params[\"preschool_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Other preschool, overall, se\", \"Nontest score, 5-14\"] = model.bse[\"preschool_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Other preschool, overall, p-value\", \"Nontest score, 5-14\"] = model.t_test(\"preschool_fixedeffect_indicator_90 = 0\").pvalue\n",
    "\n",
    "table4.loc[\"p (HS = preschool), overall\", \"Nontest score, 5-14\"] = model.f_test(\"headstart_fixedeffect_indicator_90 = preschool_fixedeffect_indicator_90\").pvalue\n",
    "\n",
    "for name in [\"Male\", \"Black\", \"lowAFQT\"]:\n",
    "    _overall_baseline_covariates = [f\"HS_{name}\",\n",
    "                                    f\"Pre_{name}\",\n",
    "                                    f\"HS_Non{name}\",\n",
    "                                    f\"Pre_Non{name}\",]\n",
    "    _overall_baseline_covariates += [\"C(AgeTest_Yr)\", \"C(Year)\"]\n",
    "    if name != \"Male\":\n",
    "        _overall_baseline_covariates += [\"Male\"]\n",
    "    model = smf.ols(\"Noncog ~ \" + \" + \".join(_overall_baseline_covariates + standard_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "    table4.loc[f\"Head Start, {name}\", \"Nontest score, 5-14\"] = model.params[f\"HS_{name}\"]\n",
    "    table4.loc[f\"Head Start, {name}, se\", \"Nontest score, 5-14\"] = model.bse[f\"HS_{name}\"]\n",
    "    table4.loc[f\"Head Start, {name}, p-value\", \"Nontest score, 5-14\"] = model.t_test(f\"HS_{name} = 0\").pvalue\n",
    "    table4.loc[f\"Head Start, Non{name}\", \"Nontest score, 5-14\"] = model.params[f\"HS_Non{name}\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, se\", \"Nontest score, 5-14\"] = model.bse[f\"HS_Non{name}\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, p-value\", \"Nontest score, 5-14\"] = model.t_test(f\"HS_Non{name} = 0\").pvalue\n",
    "\n",
    "    table4.loc[f\"p ({name}=Non{name}), HS\", \"Nontest score, 5-14\"] = model.f_test(f\"HS_{name} = HS_Non{name}\").pvalue\n",
    "\n",
    "# long term outcomes\n",
    "mask = data['Sum_Adult'].notna()\n",
    "model = smf.ols(\"Sum_Adult ~ \" + \" + \".join(overall_baseline_covariates + standard_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "table4.loc[\"Head Start, overall\", \"Long term, 19+\"] = model.params[\"headstart_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Head Start, overall, se\", \"Long term, 19+\"] = model.bse[\"headstart_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Head Start, overall, p-value\", \"Long term, 19+\"] = model.t_test(\"headstart_fixedeffect_indicator_90 = 0\").pvalue\n",
    "table4.loc[\"Other preschool, overall\", \"Long term, 19+\"] = model.params[\"preschool_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Other preschool, overall, se\", \"Long term, 19+\"] = model.bse[\"preschool_fixedeffect_indicator_90\"]\n",
    "table4.loc[\"Other preschool, overall, p-value\", \"Long term, 19+\"] = model.t_test(\"preschool_fixedeffect_indicator_90 = 0\").pvalue\n",
    "\n",
    "table4.loc[\"p (HS = preschool), overall\", \"Long term, 19+\"] = model.f_test(\"headstart_fixedeffect_indicator_90 = preschool_fixedeffect_indicator_90\").pvalue\n",
    "\n",
    "for name in [\"Male\", \"Black\", \"lowAFQT\"]:\n",
    "    _overall_baseline_covariates = [f\"HS_{name}\",\n",
    "                                    f\"Pre_{name}\",\n",
    "                                    f\"HS_Non{name}\",\n",
    "                                    f\"Pre_Non{name}\",]\n",
    "    _overall_baseline_covariates += [\"C(AgeTest_Yr)\", \"C(Year)\"]\n",
    "    if name != \"Male\":\n",
    "        _overall_baseline_covariates += [\"Male\"]\n",
    "    model = smf.ols(\"Sum_Adult ~ \" + \" + \".join(_overall_baseline_covariates + standard_covariates + family_fixed_effects), data=data.loc[mask]).fit(cov_type='cluster', cov_kwds={'groups': data.loc[mask, 'MotherID']})\n",
    "    table4.loc[f\"Head Start, {name}\", \"Long term, 19+\"] = model.params[f\"HS_{name}\"]\n",
    "    table4.loc[f\"Head Start, {name}, se\", \"Long term, 19+\"] = model.bse[f\"HS_{name}\"]\n",
    "    table4.loc[f\"Head Start, {name}, p-value\", \"Long term, 19+\"] = model.t_test(f\"HS_{name} = 0\").pvalue\n",
    "    table4.loc[f\"Head Start, Non{name}\", \"Long term, 19+\"] = model.params[f\"HS_Non{name}\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, se\", \"Long term, 19+\"] = model.bse[f\"HS_Non{name}\"]\n",
    "    table4.loc[f\"Head Start, Non{name}, p-value\", \"Long term, 19+\"] = model.t_test(f\"HS_Non{name} = 0\").pvalue\n",
    "\n",
    "    table4.loc[f\"p ({name}=Non{name}), HS\", \"Long term, 19+\"] = model.f_test(f\"HS_{name} = HS_Non{name}\").pvalue\n",
    "\n",
    "table4.to_csv(\"table4.csv\")\n",
    "table4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Std Dev</th>\n",
       "      <th>Head Start Mean</th>\n",
       "      <th>Other Preschool Mean</th>\n",
       "      <th>No Preschool Mean</th>\n",
       "      <th>Total Sample Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test_std</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.046595</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>-0.209729</td>\n",
       "      <td>0.228275</td>\n",
       "      <td>-0.043883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Noncog</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.619999</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>-0.059064</td>\n",
       "      <td>0.097838</td>\n",
       "      <td>-0.049033</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum_Adult</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.162315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.118317</td>\n",
       "      <td>0.212288</td>\n",
       "      <td>-0.115814</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0.490722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500086</td>\n",
       "      <td>0.493617</td>\n",
       "      <td>0.488844</td>\n",
       "      <td>0.489837</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reside_with_mother_0to3</th>\n",
       "      <td>0.688584</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.46324</td>\n",
       "      <td>0.688636</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.668103</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prexisting_health_conditions</th>\n",
       "      <td>0.041908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20045</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very_low_birth_weight</th>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.128769</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logBW</th>\n",
       "      <td>4.710516</td>\n",
       "      <td>4.744932</td>\n",
       "      <td>0.253003</td>\n",
       "      <td>4.698948</td>\n",
       "      <td>4.729973</td>\n",
       "      <td>4.701848</td>\n",
       "      <td>1424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_income_0to3</th>\n",
       "      <td>10.046628</td>\n",
       "      <td>10.038945</td>\n",
       "      <td>0.728076</td>\n",
       "      <td>9.813677</td>\n",
       "      <td>10.302084</td>\n",
       "      <td>10.006404</td>\n",
       "      <td>1386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log_income_at_3</th>\n",
       "      <td>9.996653</td>\n",
       "      <td>10.023771</td>\n",
       "      <td>0.90249</td>\n",
       "      <td>9.708326</td>\n",
       "      <td>10.274472</td>\n",
       "      <td>9.988104</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_born</th>\n",
       "      <td>0.423368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494262</td>\n",
       "      <td>0.434043</td>\n",
       "      <td>0.419878</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPVTat3</th>\n",
       "      <td>20.724138</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.941011</td>\n",
       "      <td>16.128571</td>\n",
       "      <td>23.758333</td>\n",
       "      <td>20.126761</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOME_Pct_0to3</th>\n",
       "      <td>40.469811</td>\n",
       "      <td>35.75</td>\n",
       "      <td>27.368728</td>\n",
       "      <td>32.503049</td>\n",
       "      <td>47.831897</td>\n",
       "      <td>37.473881</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moth_HrsWorked_BefBirth</th>\n",
       "      <td>26.164913</td>\n",
       "      <td>27.76923</td>\n",
       "      <td>11.97129</td>\n",
       "      <td>23.294815</td>\n",
       "      <td>27.697058</td>\n",
       "      <td>26.139038</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moth_HrsWorked_Avg_0to3</th>\n",
       "      <td>32.221325</td>\n",
       "      <td>35.466667</td>\n",
       "      <td>10.766403</td>\n",
       "      <td>32.54718</td>\n",
       "      <td>32.154068</td>\n",
       "      <td>32.021427</td>\n",
       "      <td>816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moth_HrsWorked_0to1</th>\n",
       "      <td>32.075661</td>\n",
       "      <td>36.833336</td>\n",
       "      <td>11.968674</td>\n",
       "      <td>31.233459</td>\n",
       "      <td>32.374584</td>\n",
       "      <td>32.419498</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Father_HH_0to3</th>\n",
       "      <td>0.635381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.446908</td>\n",
       "      <td>0.498069</td>\n",
       "      <td>0.747373</td>\n",
       "      <td>0.62276</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GMom_0to3</th>\n",
       "      <td>0.209505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329953</td>\n",
       "      <td>0.252652</td>\n",
       "      <td>0.165451</td>\n",
       "      <td>0.214133</td>\n",
       "      <td>1387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MomCare</th>\n",
       "      <td>0.64261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.413083</td>\n",
       "      <td>0.676282</td>\n",
       "      <td>0.576741</td>\n",
       "      <td>0.676934</td>\n",
       "      <td>1448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RelCare</th>\n",
       "      <td>0.196708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.337112</td>\n",
       "      <td>0.173789</td>\n",
       "      <td>0.227181</td>\n",
       "      <td>0.187885</td>\n",
       "      <td>1448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NonRelCare</th>\n",
       "      <td>0.160681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299995</td>\n",
       "      <td>0.149929</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.135181</td>\n",
       "      <td>1448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moth_Smoke_BefBirth</th>\n",
       "      <td>0.359104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479912</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.348936</td>\n",
       "      <td>0.387931</td>\n",
       "      <td>1384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alc_BefBirth</th>\n",
       "      <td>0.23506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.424459</td>\n",
       "      <td>0.291971</td>\n",
       "      <td>0.19598</td>\n",
       "      <td>0.23494</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Breastfed</th>\n",
       "      <td>0.344011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.475214</td>\n",
       "      <td>0.237581</td>\n",
       "      <td>0.444672</td>\n",
       "      <td>0.34433</td>\n",
       "      <td>1436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doctor_0to3</th>\n",
       "      <td>0.38175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486269</td>\n",
       "      <td>0.479042</td>\n",
       "      <td>0.316239</td>\n",
       "      <td>0.375</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dentist_0to3</th>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454981</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.291339</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moth_WeightChange</th>\n",
       "      <td>30.018684</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.31105</td>\n",
       "      <td>28.454546</td>\n",
       "      <td>31.476088</td>\n",
       "      <td>30.020044</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Illness_1stYr</th>\n",
       "      <td>0.528467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.49937</td>\n",
       "      <td>0.530337</td>\n",
       "      <td>0.531317</td>\n",
       "      <td>0.52381</td>\n",
       "      <td>1370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Premature</th>\n",
       "      <td>0.209913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.407394</td>\n",
       "      <td>0.214447</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.210412</td>\n",
       "      <td>1372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insurance_0to3</th>\n",
       "      <td>0.525093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.482296</td>\n",
       "      <td>0.353293</td>\n",
       "      <td>0.687234</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medicaid_0to3</th>\n",
       "      <td>0.312268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447503</td>\n",
       "      <td>0.505988</td>\n",
       "      <td>0.13617</td>\n",
       "      <td>0.378676</td>\n",
       "      <td>538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Mean     Median    Std Dev Head Start Mean  \\\n",
       "Test_std                           -0.0  -0.046595   0.999819       -0.209729   \n",
       "Noncog                              0.0   0.619999   0.999995       -0.059064   \n",
       "Sum_Adult                          -0.0   0.162315        1.0       -0.118317   \n",
       "male                           0.490722        0.0   0.500086        0.493617   \n",
       "reside_with_mother_0to3        0.688584        1.0    0.46324        0.688636   \n",
       "prexisting_health_conditions   0.041908        0.0    0.20045        0.045455   \n",
       "very_low_birth_weight          0.016854        0.0   0.128769        0.017505   \n",
       "logBW                          4.710516   4.744932   0.253003        4.698948   \n",
       "log_income_0to3               10.046628  10.038945   0.728076        9.813677   \n",
       "log_income_at_3                9.996653  10.023771    0.90249        9.708326   \n",
       "first_born                     0.423368        0.0   0.494262        0.434043   \n",
       "PPVTat3                       20.724138       17.0  12.941011       16.128571   \n",
       "HOME_Pct_0to3                 40.469811      35.75  27.368728       32.503049   \n",
       "Moth_HrsWorked_BefBirth       26.164913   27.76923   11.97129       23.294815   \n",
       "Moth_HrsWorked_Avg_0to3       32.221325  35.466667  10.766403        32.54718   \n",
       "Moth_HrsWorked_0to1           32.075661  36.833336  11.968674       31.233459   \n",
       "Father_HH_0to3                 0.635381        1.0   0.446908        0.498069   \n",
       "GMom_0to3                      0.209505        0.0   0.329953        0.252652   \n",
       "MomCare                         0.64261        1.0   0.413083        0.676282   \n",
       "RelCare                        0.196708        0.0   0.337112        0.173789   \n",
       "NonRelCare                     0.160681        0.0   0.299995        0.149929   \n",
       "Moth_Smoke_BefBirth            0.359104        0.0   0.479912            0.34   \n",
       "Alc_BefBirth                    0.23506        0.0   0.424459        0.291971   \n",
       "Breastfed                      0.344011        0.0   0.475214        0.237581   \n",
       "Doctor_0to3                     0.38175        0.0   0.486269        0.479042   \n",
       "Dentist_0to3                   0.291667        0.0   0.454981        0.371795   \n",
       "Moth_WeightChange             30.018684       28.0   15.31105       28.454546   \n",
       "Illness_1stYr                  0.528467        1.0    0.49937        0.530337   \n",
       "Premature                      0.209913        0.0   0.407394        0.214447   \n",
       "Insurance_0to3                 0.525093        0.5   0.482296        0.353293   \n",
       "Medicaid_0to3                  0.312268        0.0   0.447503        0.505988   \n",
       "\n",
       "                             Other Preschool Mean No Preschool Mean  \\\n",
       "Test_std                                 0.228275         -0.043883   \n",
       "Noncog                                   0.097838         -0.049033   \n",
       "Sum_Adult                                0.212288         -0.115814   \n",
       "male                                     0.488844          0.489837   \n",
       "reside_with_mother_0to3                  0.708333          0.668103   \n",
       "prexisting_health_conditions               0.0375          0.043103   \n",
       "very_low_birth_weight                    0.010288          0.022869   \n",
       "logBW                                    4.729973          4.701848   \n",
       "log_income_0to3                         10.302084         10.006404   \n",
       "log_income_at_3                         10.274472          9.988104   \n",
       "first_born                               0.419878          0.416667   \n",
       "PPVTat3                                 23.758333         20.126761   \n",
       "HOME_Pct_0to3                           47.831897         37.473881   \n",
       "Moth_HrsWorked_BefBirth                 27.697058         26.139038   \n",
       "Moth_HrsWorked_Avg_0to3                 32.154068         32.021427   \n",
       "Moth_HrsWorked_0to1                     32.374584         32.419498   \n",
       "Father_HH_0to3                           0.747373           0.62276   \n",
       "GMom_0to3                                0.165451          0.214133   \n",
       "MomCare                                  0.576741          0.676934   \n",
       "RelCare                                  0.227181          0.187885   \n",
       "NonRelCare                               0.196078          0.135181   \n",
       "Moth_Smoke_BefBirth                      0.348936          0.387931   \n",
       "Alc_BefBirth                              0.19598           0.23494   \n",
       "Breastfed                                0.444672           0.34433   \n",
       "Doctor_0to3                              0.316239             0.375   \n",
       "Dentist_0to3                             0.235294          0.291339   \n",
       "Moth_WeightChange                       31.476088         30.020044   \n",
       "Illness_1stYr                            0.531317           0.52381   \n",
       "Premature                                0.205128          0.210412   \n",
       "Insurance_0to3                           0.687234          0.455882   \n",
       "Medicaid_0to3                             0.13617          0.378676   \n",
       "\n",
       "                             Total Sample Size  \n",
       "Test_std                                   NaN  \n",
       "Noncog                                     NaN  \n",
       "Sum_Adult                                  NaN  \n",
       "male                                      1455  \n",
       "reside_with_mother_0to3                   1384  \n",
       "prexisting_health_conditions              1384  \n",
       "very_low_birth_weight                     1424  \n",
       "logBW                                     1424  \n",
       "log_income_0to3                           1386  \n",
       "log_income_at_3                           1160  \n",
       "first_born                                1455  \n",
       "PPVTat3                                    261  \n",
       "HOME_Pct_0to3                              530  \n",
       "Moth_HrsWorked_BefBirth                    475  \n",
       "Moth_HrsWorked_Avg_0to3                    816  \n",
       "Moth_HrsWorked_0to1                        478  \n",
       "Father_HH_0to3                             887  \n",
       "GMom_0to3                                 1387  \n",
       "MomCare                                   1448  \n",
       "RelCare                                   1448  \n",
       "NonRelCare                                1448  \n",
       "Moth_Smoke_BefBirth                       1384  \n",
       "Alc_BefBirth                               502  \n",
       "Breastfed                                 1436  \n",
       "Doctor_0to3                                537  \n",
       "Dentist_0to3                               504  \n",
       "Moth_WeightChange                         1338  \n",
       "Illness_1stYr                             1370  \n",
       "Premature                                 1372  \n",
       "Insurance_0to3                             538  \n",
       "Medicaid_0to3                              538  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for res in [\"Test_std\", \"Noncog\", \"Sum_Adult\"]:\n",
    "    mask = data[res].notna()\n",
    "    table_appendix.loc[res, \"Mean\"] = data.loc[mask, res].mean()\n",
    "    table_appendix.loc[res, \"Std Dev\"] = data.loc[mask, res].std()\n",
    "    table_appendix.loc[res, \"Median\"] = data.loc[mask, res].median()\n",
    "    mask = (data[\"headstart_fixedeffect_indicator_90\"] == 1) & data[res].notna()\n",
    "    table_appendix.loc[res, \"Head Start Mean\"] = data.loc[mask, res].mean()\n",
    "    mask = (data[\"preschool_fixedeffect_indicator_90\"] == 1) & data[res].notna()\n",
    "    table_appendix.loc[res, \"Other Preschool Mean\"] = data.loc[mask, res].mean()\n",
    "    mask = (data[\"headstart_fixedeffect_indicator_90\"] == 0) & (data[\"preschool_fixedeffect_indicator_90\"] == 0) & data[res].notna()\n",
    "    table_appendix.loc[res, \"No Preschool Mean\"] = data.loc[mask, res].mean()\n",
    "\n",
    "table_appendix.to_csv(\"table_appendix.csv\")\n",
    "table_appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat256",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
